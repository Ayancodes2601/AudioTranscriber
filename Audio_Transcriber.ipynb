{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1QWPccFa-yU6W0halO2k_pgbxMfs-x_QQ",
      "authorship_tag": "ABX9TyNKEARSqeVguLW4lvolmPQv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayancodes2601/AudioTranscriber/blob/main/Audio_Transcriber.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!sudo apt update && sudo apt install ffmpeg"
      ],
      "metadata": {
        "id": "0W8G1vG2itsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper \"/content/drive/MyDrive/Lecture 2 _ Image Classification.mp4\" --model medium.en"
      ],
      "metadata": {
        "id": "tnE3ud9YlaoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "\n",
        "def group_complete_thoughts(timestamps):\n",
        "    grouped_timestamps = []\n",
        "    current_group = []\n",
        "\n",
        "    for start_time, text in timestamps:\n",
        "        current_group.append(text)\n",
        "\n",
        "        if text[-1] in ['.', '!', '?'] and len(current_group) >= 3:\n",
        "            grouped_timestamps.append((start_time, \" \".join(current_group)))\n",
        "            current_group = []\n",
        "\n",
        "    return grouped_timestamps\n",
        "\n",
        "def format_time(time_str):\n",
        "    h, m, s = map(int, re.sub(r',\\d{3}', '', time_str).split(':'))\n",
        "    return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
        "\n",
        "def srt_to_json(srt_content):\n",
        "    json_data = {\n",
        "        \"dataset_link\": \"https://www.youtube.com/watch?v=OoUX-nOEjG0&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=2\",\n",
        "        \"dataset\": {}\n",
        "    }\n",
        "\n",
        "    pattern = r\"\\d+\\n(\\d{2}:\\d{2}:\\d{2},\\d{3}) --> (\\d{2}:\\d{2}:\\d{2},\\d{3})\\n(.+?)(?=\\n\\d+\\n\\d{2}:\\d{2}:\\d{2},\\d{3} -->|\\Z)\"\n",
        "    matches = re.findall(pattern, srt_content, re.DOTALL)\n",
        "\n",
        "    timestamps = [(format_time(match[0]), match[2].strip()) for match in matches]\n",
        "    grouped_timestamps = group_complete_thoughts(timestamps)\n",
        "\n",
        "    for i, (start_time, combined_text) in enumerate(grouped_timestamps):\n",
        "        json_data[\"dataset\"][start_time] = combined_text\n",
        "\n",
        "    return json.dumps(json_data, indent=2)\n",
        "\n",
        "# Replace this with the actual path to your SRT file\n",
        "srt_file_path = \"/content/Lecture 2 _ Image Classification.srt\"\n",
        "with open(srt_file_path, \"r\") as srt_file:\n",
        "    srt_content = srt_file.read()\n",
        "    json_data = srt_to_json(srt_content)\n",
        "\n",
        "with open(\"transcript1.json\", \"w\") as file:\n",
        "    file.write(json_data)\n"
      ],
      "metadata": {
        "id": "fRVJNAb-mHFx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}