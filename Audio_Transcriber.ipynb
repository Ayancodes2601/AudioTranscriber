{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1QWPccFa-yU6W0halO2k_pgbxMfs-x_QQ",
      "authorship_tag": "ABX9TyNKEARSqeVguLW4lvolmPQv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayancodes2601/AudioTranscriber/blob/main/Audio_Transcriber.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!sudo apt update && sudo apt install ffmpeg"
      ],
      "metadata": {
        "id": "0W8G1vG2itsh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5aa26e9-c2a2-4b37-d3ee-a14d75413be3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-78ipmktx\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-78ipmktx\n",
            "  Resolved https://github.com/openai/whisper.git to commit e8622f9afc4eba139bf796c210f5c01081000472\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (2.0.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (0.56.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (4.66.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (10.1.0)\n",
            "Collecting tiktoken==0.3.3 (from openai-whisper==20230314)\n",
            "  Downloading tiktoken-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.3->openai-whisper==20230314) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.3->openai-whisper==20230314) (2.31.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230314) (3.27.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230314) (3.12.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230314) (16.0.6)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20230314) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20230314) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230314) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230314) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230314) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230314) (3.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20230314) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20230314) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20230314-py3-none-any.whl size=798395 sha256=8e97052230f3de87e1478729fe5a8fb9bc1039d1afae6d2f7f326fa92eca0396\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-joo8iy62/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20230314 tiktoken-0.3.3\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [458 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,136 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,241 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [980 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [860 kB]\n",
            "Hit:14 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 5,017 kB in 2s (2,377 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "16 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 16 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper \"/content/drive/MyDrive/Lecture 2 _ Image Classification.mp4\" --model medium.en"
      ],
      "metadata": {
        "id": "tnE3ud9YlaoP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04da59ed-ac62-4af8-b8e6-50fc1a1284a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████████████████████████████████| 1.42G/1.42G [00:11<00:00, 130MiB/s]\n",
            "[00:00.000 --> 00:02.840]  Welcome to lecture two of CS231n.\n",
            "[00:02.840 --> 00:05.400]  On Tuesday, we, just recall, we sort of gave you\n",
            "[00:05.400 --> 00:07.800]  the big picture view of what is computer vision,\n",
            "[00:07.800 --> 00:09.840]  what is the history, and a little bit of the overview\n",
            "[00:09.840 --> 00:12.240]  of the class, and today, we're really gonna dive in\n",
            "[00:12.240 --> 00:14.040]  for the first time into the details,\n",
            "[00:14.040 --> 00:16.080]  and we'll start to see in much more depth\n",
            "[00:16.080 --> 00:18.440]  exactly how some of these learning algorithms\n",
            "[00:18.440 --> 00:20.440]  actually work in practice.\n",
            "[00:20.440 --> 00:22.520]  So the first lecture of the class is going to be\n",
            "[00:22.520 --> 00:24.840]  a little bit more detailed, and we'll start to see\n",
            "[00:24.840 --> 00:28.000]  how these learning algorithms actually work in practice.\n",
            "[00:28.040 --> 00:29.720]  So the first lecture of the class is probably\n",
            "[00:29.720 --> 00:32.040]  the sort of the largest big picture vision,\n",
            "[00:32.040 --> 00:34.000]  and the majority of the lectures in this class\n",
            "[00:34.000 --> 00:36.360]  will be much more detail-oriented, much more focused\n",
            "[00:36.360 --> 00:39.760]  on the specific mechanics of these different algorithms.\n",
            "[00:39.760 --> 00:41.440]  So today, we'll see our first learning algorithm,\n",
            "[00:41.440 --> 00:43.440]  and that'll be really exciting, I think.\n",
            "[00:43.440 --> 00:45.800]  But before we get to that, I wanted to talk about\n",
            "[00:45.800 --> 00:47.560]  a couple administrative issues.\n",
            "[00:47.560 --> 00:49.760]  One is Piazza.\n",
            "[00:49.760 --> 00:52.280]  So I saw, when I checked yesterday, it seemed like\n",
            "[00:52.280 --> 00:55.440]  we had maybe 500 students signed up on Piazza,\n",
            "[00:55.440 --> 00:56.960]  which means that there are several hundred of you\n",
            "[00:56.960 --> 00:58.600]  who are not yet there.\n",
            "[00:58.600 --> 01:01.600]  So we really want Piazza to be the main source\n",
            "[01:01.600 --> 01:04.400]  of communication between the students and the core staff.\n",
            "[01:04.400 --> 01:07.520]  So we've gotten a lot of questions to the staff list\n",
            "[01:07.520 --> 01:11.360]  about project ideas or questions about midterm attendance\n",
            "[01:11.360 --> 01:14.080]  or poster session attendance, and any sort of questions\n",
            "[01:14.080 --> 01:16.240]  like that should really go to Piazza.\n",
            "[01:16.240 --> 01:18.360]  You'll probably get answers to your questions faster\n",
            "[01:18.360 --> 01:21.520]  on Piazza because all the TAs are knowing to check that,\n",
            "[01:21.520 --> 01:23.960]  and it's sort of easy for emails to get lost in the shuffle\n",
            "[01:23.960 --> 01:26.360]  if you just send to the course list.\n",
            "[01:26.360 --> 01:29.040]  It's also come to my attention that some SCPD students\n",
            "[01:29.040 --> 01:32.480]  are having a bit of a hard time signing up for Piazza.\n",
            "[01:32.480 --> 01:35.440]  SCPD students are supposed to receive\n",
            "[01:35.440 --> 01:38.840]  a .stanford, at stanford.edu email address,\n",
            "[01:38.840 --> 01:40.760]  so once you get that email address,\n",
            "[01:40.760 --> 01:44.160]  then you can use the Stanford email to sign into Piazza.\n",
            "[01:44.160 --> 01:45.760]  Probably that doesn't affect those of you\n",
            "[01:45.760 --> 01:46.960]  who are sitting in the room right now,\n",
            "[01:46.960 --> 01:49.320]  but for those students listening on SCPD.\n",
            "[01:52.240 --> 01:55.640]  The next administrative issue is about assignment one.\n",
            "[01:55.680 --> 01:58.160]  Assignment one will be up later today,\n",
            "[01:58.160 --> 01:59.600]  probably sometime this afternoon,\n",
            "[01:59.600 --> 02:03.240]  but I promise before I go to sleep tonight, it'll be up.\n",
            "[02:03.240 --> 02:04.800]  But if you're getting a little bit antsy\n",
            "[02:04.800 --> 02:07.040]  and really want to start working on it right now,\n",
            "[02:07.040 --> 02:10.520]  then you can look at last year's version of assignment one.\n",
            "[02:10.520 --> 02:12.640]  It'll be pretty much the same content.\n",
            "[02:12.640 --> 02:15.040]  We're just reshuffling it a little bit to make it,\n",
            "[02:15.040 --> 02:17.240]  like for example, upgrading to work with Python 3\n",
            "[02:17.240 --> 02:19.040]  rather than Python 2.7,\n",
            "[02:19.040 --> 02:21.080]  and some of these minor cosmetic changes,\n",
            "[02:21.080 --> 02:22.480]  but the content of the assignment\n",
            "[02:22.480 --> 02:24.880]  will still be the same as last year.\n",
            "[02:24.880 --> 02:26.920]  So in this assignment, you'll be implementing\n",
            "[02:26.920 --> 02:28.760]  your own k-nearest neighbor classifier,\n",
            "[02:28.760 --> 02:30.840]  which we're gonna talk about in this lecture.\n",
            "[02:30.840 --> 02:33.600]  You'll also implement several different linear classifiers,\n",
            "[02:33.600 --> 02:35.800]  including the SVM and Softmax,\n",
            "[02:35.800 --> 02:38.080]  as well as a simple two-layer neural network,\n",
            "[02:38.080 --> 02:39.400]  and we'll cover all of this content\n",
            "[02:39.400 --> 02:41.040]  over the next couple of lectures.\n",
            "[02:43.200 --> 02:46.480]  So all of our assignments are using Python and NumPy.\n",
            "[02:46.480 --> 02:49.400]  If you aren't familiar with Python or NumPy,\n",
            "[02:49.400 --> 02:51.120]  then we have written a tutorial\n",
            "[02:51.120 --> 02:52.720]  that you can find on the course website\n",
            "[02:52.720 --> 02:54.400]  to try and get you up to speed.\n",
            "[02:54.880 --> 02:57.120]  But this is actually pretty important.\n",
            "[02:57.120 --> 02:59.040]  NumPy lets you write these very efficient\n",
            "[02:59.040 --> 03:01.200]  vectorized operations that lets you do\n",
            "[03:01.200 --> 03:04.080]  quite a lot of computation in just a couple lines of code.\n",
            "[03:04.080 --> 03:07.120]  So this is super important for pretty much all aspects\n",
            "[03:07.120 --> 03:09.280]  of numerical computing and machine learning\n",
            "[03:09.280 --> 03:10.480]  and everything like that,\n",
            "[03:10.480 --> 03:13.800]  is efficiently implementing these vectorized operations.\n",
            "[03:13.800 --> 03:15.920]  And you'll get a lot of practice with this\n",
            "[03:15.920 --> 03:17.200]  on the first assignment.\n",
            "[03:17.200 --> 03:19.880]  So for those of you who don't have a lot of experience\n",
            "[03:19.880 --> 03:23.440]  with MATLAB or NumPy or other types of vectorized\n",
            "[03:23.760 --> 03:26.520]  tensor computation, I recommend that you start looking\n",
            "[03:26.520 --> 03:27.640]  at this assignment pretty early,\n",
            "[03:27.640 --> 03:29.840]  and also read carefully through the tutorial.\n",
            "[03:32.200 --> 03:35.000]  The other thing I wanted to talk about is that\n",
            "[03:35.000 --> 03:37.000]  we're happy to announce that we got,\n",
            "[03:37.000 --> 03:38.920]  we're officially supported through Google Cloud\n",
            "[03:38.920 --> 03:40.280]  for this class.\n",
            "[03:40.280 --> 03:43.600]  So Google Cloud is somewhat similar to Amazon AWS.\n",
            "[03:43.600 --> 03:46.680]  You can go and start virtual machines up in the cloud.\n",
            "[03:46.680 --> 03:48.960]  These virtual machines can have GPUs.\n",
            "[03:48.960 --> 03:52.120]  So we have a, well, we're working on the tutorial\n",
            "[03:52.160 --> 03:53.920]  for exactly how to use Google Cloud\n",
            "[03:53.920 --> 03:55.520]  and get it to work for the assignments.\n",
            "[03:55.520 --> 03:58.840]  But our intention is that you'll be able to just download\n",
            "[03:58.840 --> 04:01.120]  some image and it'll be very seamless for you\n",
            "[04:01.120 --> 04:03.400]  to work on the assignment on one of these instances\n",
            "[04:03.400 --> 04:04.840]  on the cloud.\n",
            "[04:04.840 --> 04:07.720]  And because Google has very generously supported\n",
            "[04:07.720 --> 04:10.600]  this course, we'll be able to distribute to each of you\n",
            "[04:10.600 --> 04:13.600]  coupons that let you use Google Cloud credits\n",
            "[04:13.600 --> 04:15.560]  for free for the class.\n",
            "[04:15.560 --> 04:18.560]  So you can feel free to use these for the assignments\n",
            "[04:18.560 --> 04:20.120]  and also for the course projects\n",
            "[04:20.160 --> 04:22.600]  when you want to start using GPUs and larger machines\n",
            "[04:22.600 --> 04:23.960]  and whatnot.\n",
            "[04:23.960 --> 04:26.840]  So we'll post more details about that probably on Piazza\n",
            "[04:26.840 --> 04:29.080]  later today, but I just wanted to mention,\n",
            "[04:29.080 --> 04:31.320]  because I know there had been a couple of questions\n",
            "[04:31.320 --> 04:34.480]  about can I use my laptop, do I have to run on corn,\n",
            "[04:34.480 --> 04:35.880]  do I have to, whatever.\n",
            "[04:35.880 --> 04:38.320]  And the answer is that you'll be able to run on Google Cloud\n",
            "[04:38.320 --> 04:41.200]  and we'll provide you some coupons for that.\n",
            "[04:43.880 --> 04:48.040]  Yeah, so those are kind of the major administrative issues\n",
            "[04:48.040 --> 04:49.480]  I wanted to talk about today.\n",
            "[04:49.800 --> 04:52.040]  And then let's dive into the content.\n",
            "[04:53.520 --> 04:56.560]  So the last lecture we talked a little bit about this task\n",
            "[04:56.560 --> 04:59.280]  of image classification, which is really a core task\n",
            "[04:59.280 --> 05:00.400]  in computer vision.\n",
            "[05:00.400 --> 05:02.480]  And this is something that we'll really focus on\n",
            "[05:02.480 --> 05:05.560]  throughout the course of the class, is exactly how do we work\n",
            "[05:05.560 --> 05:07.760]  on this image classification task.\n",
            "[05:07.760 --> 05:10.160]  So a little bit more concretely, when you're doing\n",
            "[05:10.160 --> 05:14.360]  image classification, your system receives some input image,\n",
            "[05:14.360 --> 05:17.240]  which is this cute cat in this example, and the system\n",
            "[05:17.240 --> 05:22.240]  is aware of some predetermined set of categories or labels.\n",
            "[05:22.240 --> 05:26.200]  So these might be like a dog or a cat or a truck or a plane,\n",
            "[05:26.200 --> 05:28.920]  and there's some fixed set of category labels,\n",
            "[05:28.920 --> 05:31.200]  and the job of the computer is to look at the picture\n",
            "[05:31.200 --> 05:34.640]  and assign it one of these fixed category labels.\n",
            "[05:34.640 --> 05:36.400]  This seems like a really easy problem,\n",
            "[05:36.400 --> 05:40.120]  because so much of your own visual system and your brain\n",
            "[05:40.120 --> 05:43.160]  is hardwired for doing these sort of visual recognition\n",
            "[05:43.160 --> 05:46.440]  tasks, but this is actually a really, really hard problem\n",
            "[05:46.480 --> 05:48.200]  for a machine.\n",
            "[05:48.200 --> 05:50.720]  So if you dig in and think about actually what does\n",
            "[05:50.720 --> 05:53.120]  a computer see when it looks at this image,\n",
            "[05:53.120 --> 05:55.720]  it definitely doesn't get this holistic idea of a cat\n",
            "[05:55.720 --> 05:57.520]  that you see when you look at it.\n",
            "[05:57.520 --> 05:59.520]  And the computer really is representing the image\n",
            "[05:59.520 --> 06:01.720]  as this gigantic grid of numbers.\n",
            "[06:01.720 --> 06:06.720]  So the image might be something like 800 by 600 pixels,\n",
            "[06:07.320 --> 06:09.960]  and each pixel is represented by three numbers,\n",
            "[06:09.960 --> 06:13.040]  giving the red, green, and blue values for that pixel.\n",
            "[06:13.040 --> 06:15.800]  So to the computer, this is just a gigantic grid of numbers.\n",
            "[06:15.840 --> 06:18.440]  And it's very difficult to distill the catness\n",
            "[06:18.440 --> 06:22.560]  out of this giant array of thousands or whatever,\n",
            "[06:22.560 --> 06:24.240]  of very many different numbers.\n",
            "[06:26.920 --> 06:29.320]  So we refer to this problem as the semantic gap,\n",
            "[06:29.320 --> 06:32.720]  that this idea of a cat, or this label of a cat,\n",
            "[06:32.720 --> 06:35.520]  is a semantic label that we're assigning to this image,\n",
            "[06:35.520 --> 06:38.000]  and there's this huge gap between the semantic idea\n",
            "[06:38.000 --> 06:40.880]  of a cat and these pixel values that the computer\n",
            "[06:40.880 --> 06:41.800]  is actually seeing.\n",
            "[06:42.880 --> 06:44.240]  And this is a really hard problem,\n",
            "[06:44.240 --> 06:48.080]  because you can change the picture in very small,\n",
            "[06:48.080 --> 06:50.520]  subtle ways that will cause this pixel grid\n",
            "[06:50.520 --> 06:51.920]  to change entirely.\n",
            "[06:51.920 --> 06:54.000]  So for example, if we took this same cat,\n",
            "[06:54.000 --> 06:56.560]  and if the cat happened to sit still and not even twitch,\n",
            "[06:56.560 --> 06:59.040]  not move a muscle, which is never gonna happen,\n",
            "[06:59.040 --> 07:01.080]  but we moved the camera to the other side,\n",
            "[07:01.080 --> 07:04.080]  then every single grid, every single pixel\n",
            "[07:04.080 --> 07:06.960]  in this giant grid of numbers would be completely different.\n",
            "[07:06.960 --> 07:09.520]  But somehow, it's still representing the same cat.\n",
            "[07:09.520 --> 07:11.680]  And our algorithms need to be robust to this.\n",
            "[07:12.680 --> 07:14.960]  But not only viewpoint is one problem,\n",
            "[07:14.960 --> 07:16.080]  another is illumination.\n",
            "[07:16.080 --> 07:17.560]  There can be different lighting conditions\n",
            "[07:17.560 --> 07:19.080]  going on in the scene.\n",
            "[07:19.080 --> 07:21.200]  Whether the cat is appearing in this very dark,\n",
            "[07:21.200 --> 07:24.480]  moody scene, or in this very bright, sunlit scene,\n",
            "[07:24.480 --> 07:26.560]  it's still a cat, and our algorithms need to be robust\n",
            "[07:26.560 --> 07:27.400]  to that.\n",
            "[07:28.400 --> 07:30.360]  Objects can also deform.\n",
            "[07:30.360 --> 07:32.400]  I think cats are maybe among the more deformable\n",
            "[07:32.400 --> 07:34.480]  of animals that you might see out there.\n",
            "[07:34.480 --> 07:36.800]  And cats can really assume a lot of different\n",
            "[07:36.800 --> 07:39.560]  varied poses and positions, and our algorithms\n",
            "[07:39.560 --> 07:42.400]  should be robust to these different kinds of transforms.\n",
            "[07:43.400 --> 07:45.720]  There can also be problems of occlusion,\n",
            "[07:45.720 --> 07:48.600]  where you might only see part of a cat,\n",
            "[07:48.600 --> 07:51.000]  like just the face, or in this extreme example,\n",
            "[07:51.000 --> 07:53.960]  just a tail peeking out from under the couch cushion.\n",
            "[07:53.960 --> 07:56.840]  But in these cases, it's pretty easy for you as a person\n",
            "[07:56.840 --> 07:58.880]  to realize that this is probably a cat,\n",
            "[07:58.880 --> 08:01.840]  and you still recognize these images as cats.\n",
            "[08:01.840 --> 08:04.440]  And this is something that our algorithms also\n",
            "[08:04.440 --> 08:07.400]  must be robust to, which is quite difficult, I think.\n",
            "[08:08.400 --> 08:10.760]  There can also be problems of background clutter,\n",
            "[08:10.760 --> 08:13.360]  where maybe the foreground object, the cat,\n",
            "[08:13.360 --> 08:15.840]  could actually look quite similar in appearance\n",
            "[08:15.840 --> 08:18.000]  to the background, and this is another thing\n",
            "[08:18.000 --> 08:19.200]  that we need to handle.\n",
            "[08:20.320 --> 08:23.600]  There's also this problem of intra-class variation,\n",
            "[08:23.600 --> 08:26.720]  that this one notion of catness actually spans\n",
            "[08:26.720 --> 08:28.480]  a lot of different visual appearances,\n",
            "[08:28.480 --> 08:30.640]  and cats can come in different shapes and sizes\n",
            "[08:30.640 --> 08:33.560]  and colors and ages, and our algorithm, again,\n",
            "[08:33.560 --> 08:36.480]  needs to work and handle all these different variations.\n",
            "[08:36.480 --> 08:40.080]  So this is actually a really, really challenging problem.\n",
            "[08:40.080 --> 08:43.600]  And it's sort of easy to forget how easy this is,\n",
            "[08:43.600 --> 08:46.400]  because so much of your brain is specifically tuned\n",
            "[08:46.400 --> 08:47.920]  for dealing with these things.\n",
            "[08:47.920 --> 08:49.760]  But now, if we want our computer programs\n",
            "[08:49.760 --> 08:52.640]  to deal with all of these problems all simultaneously,\n",
            "[08:52.640 --> 08:54.320]  and not just for cats, by the way,\n",
            "[08:54.320 --> 08:56.920]  but for just about any object category you could imagine,\n",
            "[08:56.920 --> 08:59.080]  this is a fantastically challenging problem.\n",
            "[08:59.080 --> 09:00.760]  And it's actually somewhat miraculous\n",
            "[09:00.760 --> 09:03.040]  that this works at all, in my opinion.\n",
            "[09:03.040 --> 09:04.840]  But actually, not only does it work,\n",
            "[09:04.880 --> 09:07.280]  but these things work very close to human accuracy\n",
            "[09:07.280 --> 09:10.880]  in some limited situations, and take maybe only hundreds\n",
            "[09:10.880 --> 09:12.440]  of milliseconds to do so.\n",
            "[09:12.440 --> 09:14.920]  So this is some pretty amazing, incredible technology,\n",
            "[09:14.920 --> 09:18.400]  in my opinion, and over the course of the rest of the class,\n",
            "[09:18.400 --> 09:20.520]  we'll really see what kinds of advancements\n",
            "[09:20.520 --> 09:21.600]  have made this possible.\n",
            "[09:23.720 --> 09:25.920]  So now, if you kind of think about what is the API\n",
            "[09:25.920 --> 09:28.560]  for writing an image classifier, you might sit down\n",
            "[09:28.560 --> 09:31.120]  and try to write a method in Python like this,\n",
            "[09:31.120 --> 09:32.760]  where you wanna take in an image,\n",
            "[09:32.760 --> 09:35.240]  and then do some crazy magic, and then eventually spit out\n",
            "[09:35.240 --> 09:38.200]  this class label to say cat or dog or whatnot.\n",
            "[09:38.200 --> 09:41.480]  And there's really no obvious way to do this, right?\n",
            "[09:41.480 --> 09:43.480]  Like, if you're taking an algorithms class,\n",
            "[09:43.480 --> 09:46.840]  and your task is to sort numbers or compute a convex hull,\n",
            "[09:46.840 --> 09:49.720]  or even do something like RSA encryption,\n",
            "[09:49.720 --> 09:51.560]  you sort of can write down an algorithm\n",
            "[09:51.560 --> 09:53.840]  and enumerate all the steps that need to happen\n",
            "[09:53.840 --> 09:56.000]  in order for these things to work.\n",
            "[09:56.000 --> 09:58.400]  But when we're trying to recognize objects,\n",
            "[09:58.400 --> 10:01.560]  or recognize cats or images, there's no really clear\n",
            "[10:01.600 --> 10:04.880]  explicit algorithm that makes intuitive sense\n",
            "[10:04.880 --> 10:07.880]  for how you might go about recognizing these objects.\n",
            "[10:07.880 --> 10:10.720]  So this is, again, quite challenging if you think about,\n",
            "[10:10.720 --> 10:12.880]  like, if you knew nothing, if it was your first day\n",
            "[10:12.880 --> 10:15.520]  programming, and you had to sit down and write this function,\n",
            "[10:15.520 --> 10:17.520]  I think most people would be in trouble.\n",
            "[10:19.040 --> 10:20.920]  That being said, people have definitely made\n",
            "[10:20.920 --> 10:25.000]  explicit attempts to try to write sort of hand-coded rules\n",
            "[10:25.000 --> 10:26.880]  for recognizing different animals.\n",
            "[10:26.880 --> 10:29.640]  So we touched on this a little bit in the last lecture,\n",
            "[10:29.640 --> 10:32.240]  but maybe one idea for cats is that, you know,\n",
            "[10:32.240 --> 10:35.680]  we know that cats have ears and eyes and mouths and noses,\n",
            "[10:35.680 --> 10:38.000]  and we know that edges are, from Hubel and Wiesel,\n",
            "[10:38.000 --> 10:39.640]  we know that edges are pretty important\n",
            "[10:39.640 --> 10:41.600]  when it comes to visual recognition.\n",
            "[10:41.600 --> 10:43.840]  So one thing we might try to do is\n",
            "[10:43.840 --> 10:45.440]  compute the edges of this image,\n",
            "[10:45.440 --> 10:47.080]  and then go in and try to categorize\n",
            "[10:47.080 --> 10:48.920]  all the different corners and boundaries,\n",
            "[10:48.920 --> 10:51.560]  and say that, you know, if we have maybe three lines\n",
            "[10:51.560 --> 10:53.160]  meeting this way, then it might be a corner,\n",
            "[10:53.160 --> 10:55.360]  and an ear has one corner here and one corner there\n",
            "[10:55.360 --> 10:57.680]  and one corner there, and then kind of write down\n",
            "[10:57.680 --> 11:00.520]  this explicit set of rules for recognizing cats.\n",
            "[11:01.520 --> 11:04.400]  But this turns out not to work very well.\n",
            "[11:04.400 --> 11:07.680]  One, it's super brittle, and two, say,\n",
            "[11:07.680 --> 11:10.320]  if you want to start over for another object category,\n",
            "[11:10.320 --> 11:12.960]  and maybe not worry about cats, but talk about trucks\n",
            "[11:12.960 --> 11:15.120]  or dogs or fishes or something else,\n",
            "[11:15.120 --> 11:17.000]  then you need to start all over again.\n",
            "[11:17.000 --> 11:19.760]  So this is really not a very scalable approach.\n",
            "[11:19.760 --> 11:22.680]  We want to come up with some algorithm or some method\n",
            "[11:22.680 --> 11:26.160]  for these recognition tasks, which scales much more\n",
            "[11:26.280 --> 11:30.440]  naturally to all the variety of objects in the world.\n",
            "[11:31.440 --> 11:34.840]  So the insight that sort of makes this all work\n",
            "[11:34.840 --> 11:37.560]  is this idea of the data-driven approach,\n",
            "[11:37.560 --> 11:40.320]  is that rather than sitting down and writing\n",
            "[11:40.320 --> 11:42.920]  these hand-specified rules to try to craft\n",
            "[11:42.920 --> 11:46.080]  exactly what is a cat or a fish or what have you,\n",
            "[11:46.080 --> 11:48.120]  instead, we'll go out onto the internet\n",
            "[11:48.120 --> 11:51.480]  and collect a large data set of many, many cats\n",
            "[11:51.480 --> 11:53.720]  and many, many airplanes and many, many deer\n",
            "[11:53.720 --> 11:55.680]  and different things like this.\n",
            "[11:56.240 --> 11:58.960]  And we can actually use tools like Google Image Search\n",
            "[11:58.960 --> 12:00.840]  or something like that to go out and collect\n",
            "[12:00.840 --> 12:02.440]  a very large number of examples\n",
            "[12:02.440 --> 12:04.840]  of these different categories.\n",
            "[12:04.840 --> 12:07.040]  By the way, this actually takes quite a lot of effort\n",
            "[12:07.040 --> 12:09.040]  to go out and actually collect these data sets,\n",
            "[12:09.040 --> 12:11.360]  but people, luckily, there's a lot of really good\n",
            "[12:11.360 --> 12:14.880]  high-quality data sets out there already for you to use.\n",
            "[12:14.880 --> 12:16.760]  Then once we get this data set,\n",
            "[12:16.760 --> 12:19.520]  we train this machine learning classifier\n",
            "[12:19.520 --> 12:21.680]  that is gonna ingest all of the data,\n",
            "[12:21.680 --> 12:24.560]  summarize it in some way, and then spit out a model\n",
            "[12:24.560 --> 12:27.200]  that summarizes the knowledge of how to recognize\n",
            "[12:27.200 --> 12:29.280]  these different object categories.\n",
            "[12:29.280 --> 12:30.920]  Then finally, we'll use this train model\n",
            "[12:30.920 --> 12:33.800]  and apply it on new images that will then\n",
            "[12:33.800 --> 12:36.760]  be able to recognize cats and dogs and whatnot.\n",
            "[12:36.760 --> 12:39.240]  So here, our API has changed a little bit.\n",
            "[12:39.240 --> 12:41.560]  Rather than a single function that just inputs an image\n",
            "[12:41.560 --> 12:44.360]  and recognizes a cat, we have these two functions.\n",
            "[12:44.360 --> 12:48.240]  One that's called train, that's gonna input images\n",
            "[12:48.240 --> 12:50.120]  and labels and then output a model,\n",
            "[12:50.120 --> 12:52.800]  and then separately, another function called predict,\n",
            "[12:52.800 --> 12:53.920]  which will input the model\n",
            "[12:53.920 --> 12:56.400]  and then make predictions for images.\n",
            "[12:56.400 --> 12:58.160]  This is kind of the key insight that allowed\n",
            "[12:58.160 --> 13:00.440]  all these things to start working really well\n",
            "[13:00.440 --> 13:02.280]  over the last 10, 20 years or so.\n",
            "[13:07.080 --> 13:09.120]  This class is primarily about neural networks\n",
            "[13:09.120 --> 13:10.520]  and convolutional neural networks\n",
            "[13:10.520 --> 13:11.920]  and deep learning and all that,\n",
            "[13:11.920 --> 13:14.560]  but this idea of a data-driven approach\n",
            "[13:14.560 --> 13:16.840]  is much more general than just deep learning.\n",
            "[13:16.840 --> 13:19.920]  I think it's useful to sort of step through this process\n",
            "[13:19.920 --> 13:21.680]  for a very simple classifier first,\n",
            "[13:21.680 --> 13:23.880]  before we get to these big complex ones.\n",
            "[13:23.880 --> 13:27.760]  So probably the simplest classifier you can imagine\n",
            "[13:27.760 --> 13:29.680]  is something we call nearest neighbor.\n",
            "[13:29.680 --> 13:32.160]  The algorithm is pretty dumb, honestly.\n",
            "[13:32.160 --> 13:35.080]  So during the training step, we won't do anything.\n",
            "[13:35.080 --> 13:37.680]  We'll just memorize all of the training data.\n",
            "[13:37.680 --> 13:39.840]  So this is very simple.\n",
            "[13:39.840 --> 13:41.960]  And now during the prediction step,\n",
            "[13:41.960 --> 13:43.960]  we're gonna take some new image\n",
            "[13:43.960 --> 13:46.480]  and go and try to find the most similar image\n",
            "[13:46.480 --> 13:48.720]  in the training data to that new image\n",
            "[13:48.720 --> 13:52.200]  and now predict the label of that most similar image.\n",
            "[13:52.200 --> 13:54.840]  Very simple algorithm, but it sort of has\n",
            "[13:54.840 --> 13:56.000]  a lot of these nice properties\n",
            "[13:56.000 --> 13:58.200]  with respect to data-drivenness and whatnot.\n",
            "[14:00.720 --> 14:02.440]  So to be a little bit more concrete,\n",
            "[14:02.440 --> 14:05.720]  you might imagine working on this data set called CIFAR-10,\n",
            "[14:05.720 --> 14:08.320]  which is very commonly used in machine learning\n",
            "[14:08.320 --> 14:10.040]  as kind of a small test case,\n",
            "[14:10.040 --> 14:12.320]  and you'll be working with this data set on your homework.\n",
            "[14:12.320 --> 14:15.840]  So the CIFAR-10 data set gives you 10 different classes,\n",
            "[14:15.840 --> 14:18.440]  airplanes and automobiles and birds and cats\n",
            "[14:18.440 --> 14:20.680]  and different things like that.\n",
            "[14:20.680 --> 14:23.000]  And for each of those 10 categories,\n",
            "[14:23.000 --> 14:28.000]  it provides 10,000, sorry, it provides 50,000 training images\n",
            "[14:28.000 --> 14:31.200]  roughly evenly distributed across these 10 categories\n",
            "[14:31.200 --> 14:34.440]  and then 10,000 additional testing images\n",
            "[14:34.440 --> 14:37.080]  that you are supposed to test your algorithm on.\n",
            "[14:38.320 --> 14:40.640]  So now if you think about, so here's an example\n",
            "[14:40.640 --> 14:43.480]  of applying this simple nearest neighbor classifier\n",
            "[14:43.480 --> 14:46.560]  to some of these test images on CIFAR-10.\n",
            "[14:46.560 --> 14:51.560]  So on this grid on the right, each for the leftmost column\n",
            "[14:51.640 --> 14:54.440]  gives a test image in the CIFAR-10 data set.\n",
            "[14:54.440 --> 14:57.640]  And now on the right, we see,\n",
            "[14:57.640 --> 14:59.200]  we've sorted the training images\n",
            "[14:59.200 --> 15:02.120]  and show the most similar training images\n",
            "[15:02.120 --> 15:04.240]  to each of these test examples.\n",
            "[15:04.240 --> 15:06.680]  And you can see that they look kind of visually similar\n",
            "[15:06.680 --> 15:08.920]  to the training images,\n",
            "[15:08.920 --> 15:11.800]  although they are not always correct, right?\n",
            "[15:11.800 --> 15:14.360]  So maybe on the second row, we see that the testing,\n",
            "[15:14.360 --> 15:15.520]  this is kind of hard to see\n",
            "[15:15.520 --> 15:18.160]  because these images are 32 by 32 pixels.\n",
            "[15:18.160 --> 15:19.600]  You need to really dive in there\n",
            "[15:19.600 --> 15:22.000]  and try to make your best guess.\n",
            "[15:22.000 --> 15:23.560]  But this image is a dog,\n",
            "[15:23.560 --> 15:26.160]  and its nearest neighbor is also a dog.\n",
            "[15:26.160 --> 15:29.000]  But this next one I think is actually a deer\n",
            "[15:29.000 --> 15:30.840]  or a horse or something else.\n",
            "[15:30.840 --> 15:33.800]  But you can see that it looks quite visually similar\n",
            "[15:33.800 --> 15:35.280]  because there's kind of a white blob\n",
            "[15:35.280 --> 15:37.120]  in the middle and whatnot.\n",
            "[15:37.120 --> 15:39.600]  So if we're applying the nearest neighbor algorithm\n",
            "[15:39.600 --> 15:42.320]  to this image, we'll find the closest example\n",
            "[15:42.320 --> 15:43.400]  in the training set.\n",
            "[15:43.400 --> 15:45.880]  And now the closest example, we know it's label\n",
            "[15:45.880 --> 15:47.880]  because it comes from the training set.\n",
            "[15:47.880 --> 15:50.080]  And now we'll simply say that this testing image\n",
            "[15:50.080 --> 15:51.920]  is also a dog.\n",
            "[15:51.920 --> 15:54.000]  You can see kind of from these examples\n",
            "[15:54.000 --> 15:56.640]  that this is probably not gonna work very well,\n",
            "[15:56.640 --> 15:59.320]  but it's still kind of a nice example to work through.\n",
            "[16:01.640 --> 16:04.440]  But then one detail that we need to know is\n",
            "[16:04.440 --> 16:07.720]  given a pair of images, how can we actually compare them?\n",
            "[16:07.720 --> 16:09.520]  Because if we're gonna take our test image\n",
            "[16:09.520 --> 16:11.360]  and compare it to all the training images,\n",
            "[16:11.360 --> 16:12.920]  we actually have many different choices\n",
            "[16:12.920 --> 16:16.440]  for exactly what that comparison function should look like.\n",
            "[16:16.440 --> 16:18.520]  So in the example in the previous slide,\n",
            "[16:18.520 --> 16:21.040]  we've used what's called the L1 distance,\n",
            "[16:21.040 --> 16:23.320]  also sometimes called the Manhattan distance.\n",
            "[16:23.320 --> 16:25.600]  So this is a really, really sort of simple,\n",
            "[16:25.600 --> 16:28.200]  easy idea for comparing images.\n",
            "[16:28.200 --> 16:30.720]  And that's that we're gonna take the,\n",
            "[16:30.720 --> 16:33.680]  just compare individual pixels in these images.\n",
            "[16:33.680 --> 16:36.480]  So supposing that our test image is maybe\n",
            "[16:36.480 --> 16:40.000]  just a tiny four by four image of pixel values.\n",
            "[16:40.000 --> 16:42.840]  Then we're gonna take this upper left hand pixel\n",
            "[16:42.840 --> 16:45.040]  of the test image, subtract off the value\n",
            "[16:45.040 --> 16:47.120]  in the training image, take the absolute value,\n",
            "[16:47.120 --> 16:48.560]  and get the difference in that pixel\n",
            "[16:48.560 --> 16:49.800]  between the two images.\n",
            "[16:49.800 --> 16:51.760]  And then sum all these up across all the pixels\n",
            "[16:51.760 --> 16:52.680]  in the image.\n",
            "[16:52.680 --> 16:55.040]  So this is kind of a stupid way to compare images,\n",
            "[16:55.040 --> 16:58.720]  but it does some reasonable things sometimes.\n",
            "[16:58.720 --> 17:00.400]  But this gives us a very concrete way\n",
            "[17:00.400 --> 17:02.760]  to measure the difference between two images.\n",
            "[17:02.760 --> 17:05.840]  And in this case, we have this difference of 456\n",
            "[17:05.920 --> 17:08.000]  between these two images.\n",
            "[17:09.320 --> 17:12.560]  So here's some full Python code for implementing\n",
            "[17:12.560 --> 17:14.120]  this nearest neighbor classifier.\n",
            "[17:14.120 --> 17:16.240]  And you can see it's actually pretty short\n",
            "[17:16.240 --> 17:18.400]  and pretty concise, because we've made use\n",
            "[17:18.400 --> 17:22.200]  of many of these vectorized operations offered by NumPy.\n",
            "[17:22.200 --> 17:24.760]  So here we can see that the training,\n",
            "[17:24.760 --> 17:27.200]  this train function that we talked about earlier\n",
            "[17:27.200 --> 17:29.840]  is, again, very simple in the case of nearest neighbor.\n",
            "[17:29.840 --> 17:31.440]  You just memorize the training data.\n",
            "[17:31.440 --> 17:34.280]  There's not really much to do here.\n",
            "[17:34.320 --> 17:36.800]  And now at test time, we're gonna take in our image\n",
            "[17:36.800 --> 17:40.120]  and then go in and compare using this L1 distance function,\n",
            "[17:40.120 --> 17:42.920]  our test image, to each of these training examples\n",
            "[17:42.920 --> 17:46.320]  and find the most similar example in the training set.\n",
            "[17:46.320 --> 17:48.560]  And you can see that we're actually able to do this\n",
            "[17:48.560 --> 17:51.680]  in just one or two lines of Python code\n",
            "[17:51.680 --> 17:54.800]  by utilizing these vectorized operations in NumPy.\n",
            "[17:54.800 --> 17:56.840]  So this is something that you'll get practice with\n",
            "[17:56.840 --> 17:57.960]  on the first assignment.\n",
            "[17:59.520 --> 18:03.160]  So now a couple questions about this simple classifier.\n",
            "[18:03.160 --> 18:05.760]  First, if we have n examples in our training set,\n",
            "[18:05.760 --> 18:08.520]  then how fast can we expect training and testing to be?\n",
            "[18:13.080 --> 18:14.920]  Well, training is probably constant,\n",
            "[18:14.920 --> 18:16.640]  because we don't really need to do anything.\n",
            "[18:16.640 --> 18:19.040]  We just need to memorize the data.\n",
            "[18:19.040 --> 18:20.280]  And if you're just copying a pointer,\n",
            "[18:20.280 --> 18:21.640]  that's gonna be constant time,\n",
            "[18:21.640 --> 18:23.640]  no matter how big your data set is.\n",
            "[18:23.640 --> 18:27.440]  But now at test time, we need to do this comparison step\n",
            "[18:27.440 --> 18:30.240]  and compare our test image to each of the n training\n",
            "[18:30.240 --> 18:32.040]  examples in the data set.\n",
            "[18:32.040 --> 18:33.840]  And this is actually quite slow.\n",
            "[18:35.840 --> 18:38.400]  So this is actually somewhat backwards,\n",
            "[18:38.400 --> 18:39.520]  if you think about it.\n",
            "[18:39.520 --> 18:42.720]  Because in practice, we want our classifiers\n",
            "[18:42.720 --> 18:46.200]  to be slow at training time and then fast at testing time.\n",
            "[18:46.200 --> 18:48.840]  Because you might imagine that a classifier might go\n",
            "[18:48.840 --> 18:50.920]  and be trained in a data center somewhere,\n",
            "[18:50.920 --> 18:52.960]  and you can afford to spend a lot of computation\n",
            "[18:52.960 --> 18:55.800]  at training time to make the classifier really good.\n",
            "[18:55.800 --> 18:58.600]  But then when you go and deploy the classifier at test time,\n",
            "[18:58.600 --> 19:01.360]  you want it to run on your mobile phone or in the browser\n",
            "[19:01.360 --> 19:03.200]  or some other low-power device,\n",
            "[19:03.200 --> 19:05.640]  and you really want the testing time performance\n",
            "[19:05.640 --> 19:07.960]  of your classifier to be quite fast.\n",
            "[19:07.960 --> 19:10.880]  So from this perspective, this nearest neighbor algorithm\n",
            "[19:10.880 --> 19:12.720]  is actually a little bit backwards.\n",
            "[19:12.720 --> 19:15.120]  And we'll see that once we move to convolutional\n",
            "[19:15.120 --> 19:17.740]  neural networks and other types of parametric models,\n",
            "[19:17.740 --> 19:19.200]  there'll be the reverse of this,\n",
            "[19:19.200 --> 19:21.320]  where you'll spend a lot of compute at training time,\n",
            "[19:21.320 --> 19:23.680]  but then they'll be quite fast at testing time.\n",
            "[19:25.760 --> 19:28.200]  So then the question is, what exactly does this\n",
            "[19:28.200 --> 19:29.920]  nearest neighbor algorithm look like\n",
            "[19:29.920 --> 19:31.600]  when you apply it in practice?\n",
            "[19:31.600 --> 19:35.000]  So here we've drawn what we call the decision regions\n",
            "[19:35.000 --> 19:37.080]  of a nearest neighbor classifier.\n",
            "[19:37.080 --> 19:40.560]  So here our training set consists of these points\n",
            "[19:40.560 --> 19:42.960]  in the two-dimensional plane,\n",
            "[19:42.960 --> 19:46.360]  where the color of the point represents the category\n",
            "[19:46.360 --> 19:48.480]  or the class label of that point.\n",
            "[19:48.480 --> 19:50.280]  So here we see here we have five classes\n",
            "[19:50.280 --> 19:52.480]  and some blue ones up in the corner here,\n",
            "[19:52.480 --> 19:54.720]  some purple ones in the upper right-hand corner.\n",
            "[19:54.720 --> 19:57.580]  And now for each pixel in this entire plane,\n",
            "[19:57.620 --> 20:00.780]  we've gone and computed what is the nearest training,\n",
            "[20:00.780 --> 20:03.620]  what is the nearest example in these training data,\n",
            "[20:03.620 --> 20:05.420]  and then colored the point of the background\n",
            "[20:05.420 --> 20:07.900]  corresponding to what is the class label.\n",
            "[20:07.900 --> 20:09.980]  So you can see that this nearest neighbor classifier\n",
            "[20:09.980 --> 20:11.980]  is just sort of carving up the space\n",
            "[20:11.980 --> 20:14.980]  and coloring the space according to the nearby points.\n",
            "[20:16.220 --> 20:19.260]  But this classifier is maybe not so great,\n",
            "[20:19.260 --> 20:20.940]  and by looking at this picture,\n",
            "[20:20.940 --> 20:22.660]  we can start to see some of the problems\n",
            "[20:22.660 --> 20:25.620]  that might come out with a nearest neighbor classifier.\n",
            "[20:25.620 --> 20:28.580]  For one, this central region actually contains\n",
            "[20:28.580 --> 20:30.100]  mostly green points,\n",
            "[20:30.100 --> 20:32.820]  but one little yellow point in the middle.\n",
            "[20:32.820 --> 20:35.340]  But because we're just looking at the nearest neighbor,\n",
            "[20:35.340 --> 20:37.420]  this causes a little yellow island to appear\n",
            "[20:37.420 --> 20:39.500]  in this middle of the green cluster,\n",
            "[20:39.500 --> 20:41.340]  and that's maybe not so great.\n",
            "[20:41.340 --> 20:45.020]  Maybe those points actually should have been green.\n",
            "[20:45.020 --> 20:47.620]  And then similarly, we also see these sort of fingers\n",
            "[20:47.620 --> 20:51.180]  of the green region pushing into the blue region,\n",
            "[20:51.180 --> 20:53.540]  again, due to the presence of one point,\n",
            "[20:53.580 --> 20:56.060]  which may have been noisy or spurious.\n",
            "[20:56.060 --> 20:59.220]  So this kind of motivates a slight generalization\n",
            "[20:59.220 --> 21:02.500]  of this algorithm called k-nearest neighbors.\n",
            "[21:02.500 --> 21:05.980]  So rather than just looking for the single nearest neighbor,\n",
            "[21:05.980 --> 21:08.940]  instead, we'll do something a little bit fancier\n",
            "[21:08.940 --> 21:11.580]  and find k of our nearest neighbors\n",
            "[21:11.580 --> 21:13.180]  according to our distance metric,\n",
            "[21:13.180 --> 21:15.940]  and then take a vote among each of our neighbors,\n",
            "[21:15.940 --> 21:19.580]  and then predict the majority vote among our neighbors.\n",
            "[21:19.580 --> 21:21.940]  You can imagine slightly more complex ways of doing this.\n",
            "[21:21.940 --> 21:23.900]  Maybe you vote, waited on the distance,\n",
            "[21:23.900 --> 21:25.260]  or something like that,\n",
            "[21:25.260 --> 21:28.860]  but the simplest thing that tends to work pretty well\n",
            "[21:28.860 --> 21:30.700]  is just taking a majority vote.\n",
            "[21:30.700 --> 21:33.740]  So here we've shown the exact same set of points\n",
            "[21:33.740 --> 21:36.980]  using this k equals one nearest neighbor classifier,\n",
            "[21:36.980 --> 21:39.300]  as well as k equals three and k equals five\n",
            "[21:39.300 --> 21:40.820]  in the middle and on the right.\n",
            "[21:40.820 --> 21:43.620]  And once we move to k equals three,\n",
            "[21:43.620 --> 21:45.620]  you can see that that spurious yellow point\n",
            "[21:45.620 --> 21:47.180]  in the middle of the green cluster\n",
            "[21:47.180 --> 21:50.340]  is no longer causing the points near that region\n",
            "[21:50.340 --> 21:51.900]  to be classified as yellow.\n",
            "[21:51.900 --> 21:54.700]  But now this entire green portion in the middle\n",
            "[21:54.700 --> 21:56.940]  is all being classified as green.\n",
            "[21:56.940 --> 21:58.740]  You can also see that these fingers\n",
            "[21:58.740 --> 22:00.300]  of the red and blue regions\n",
            "[22:00.300 --> 22:01.820]  are starting to get smoothed out\n",
            "[22:01.820 --> 22:03.460]  due to this majority voting.\n",
            "[22:03.460 --> 22:06.380]  And then once we move to the k equals five case,\n",
            "[22:06.380 --> 22:07.940]  then these decision boundaries\n",
            "[22:07.940 --> 22:09.620]  between the blue and red regions\n",
            "[22:09.620 --> 22:11.780]  have become quite smooth and quite nice.\n",
            "[22:11.780 --> 22:13.140]  So this is generally something,\n",
            "[22:13.140 --> 22:15.740]  so generally when you're using nearest neighbor classifiers,\n",
            "[22:15.740 --> 22:19.660]  you almost always want to use some value of k,\n",
            "[22:19.660 --> 22:21.740]  which is larger than one,\n",
            "[22:21.740 --> 22:24.300]  because this tends to smooth out your decision boundaries\n",
            "[22:24.300 --> 22:26.540]  and lead to better results.\n",
            "[22:27.860 --> 22:31.360]  So if we, to kind of, oh, yeah, question?\n",
            "[22:35.180 --> 22:36.220]  Yeah, so the question is,\n",
            "[22:36.220 --> 22:38.220]  what is the deal with these white regions?\n",
            "[22:38.220 --> 22:41.500]  And those are, the white regions are where there was\n",
            "[22:41.500 --> 22:44.140]  no majority among the k nearest neighbors.\n",
            "[22:44.140 --> 22:46.460]  You could imagine maybe doing something slightly fancier\n",
            "[22:46.460 --> 22:49.660]  and maybe taking a guess or randomly selecting\n",
            "[22:49.660 --> 22:51.540]  among the majority winners.\n",
            "[22:51.540 --> 22:53.620]  But for this simple example, we're just coloring it white\n",
            "[22:53.620 --> 22:55.260]  to indicate that there was no nearest neighbor\n",
            "[22:55.260 --> 22:56.580]  in those points.\n",
            "[22:58.820 --> 23:01.140]  So we already kind of saw, so I like to,\n",
            "[23:01.140 --> 23:03.380]  whenever we're thinking about computer vision,\n",
            "[23:03.380 --> 23:05.820]  I think it's really useful to kind of flip back and forth\n",
            "[23:05.820 --> 23:07.580]  between several different viewpoints.\n",
            "[23:07.580 --> 23:10.660]  One is this idea of high-dimensional points in the plane,\n",
            "[23:10.660 --> 23:13.900]  and then the other is actually looking at concrete images,\n",
            "[23:13.900 --> 23:16.300]  because the pixels of the image actually allow us\n",
            "[23:16.300 --> 23:20.020]  to think of these images as high-dimensional vectors.\n",
            "[23:20.060 --> 23:22.180]  And it's sort of useful to ping-pong back and forth\n",
            "[23:22.180 --> 23:24.300]  between these two different viewpoints.\n",
            "[23:24.300 --> 23:27.180]  So then, when sort of taking this k nearest neighbor\n",
            "[23:27.180 --> 23:28.780]  and going back to the images,\n",
            "[23:28.780 --> 23:30.420]  you can see that it's actually not very good.\n",
            "[23:30.420 --> 23:32.980]  Here I've colored in red and green which images\n",
            "[23:32.980 --> 23:35.060]  would actually be classified correctly or incorrectly\n",
            "[23:35.060 --> 23:36.580]  according to their nearest neighbor.\n",
            "[23:36.580 --> 23:39.220]  And you can see that it's really not very good.\n",
            "[23:39.220 --> 23:42.460]  But maybe if we used a larger value of k,\n",
            "[23:42.460 --> 23:44.700]  then this would involve actually voting among\n",
            "[23:44.700 --> 23:46.700]  maybe the top three or the top five,\n",
            "[23:46.700 --> 23:48.220]  or maybe even the whole row.\n",
            "[23:48.220 --> 23:49.740]  And you could imagine that that would end up being\n",
            "[23:50.500 --> 23:52.500]  a lot more robust to some of this noise that we see\n",
            "[23:52.500 --> 23:54.540]  when retrieving neighbors in this way.\n",
            "[23:57.500 --> 24:00.500]  So another choice we have when we're working\n",
            "[24:00.500 --> 24:02.180]  with the k nearest neighbor algorithm\n",
            "[24:02.180 --> 24:04.780]  is determining exactly how we should be comparing\n",
            "[24:04.780 --> 24:06.420]  our different points.\n",
            "[24:06.420 --> 24:08.980]  For the example so far, we've just shown this,\n",
            "[24:10.300 --> 24:12.180]  we've talked about this L1 distance,\n",
            "[24:12.180 --> 24:14.100]  which takes the sum of the absolute values\n",
            "[24:14.100 --> 24:15.780]  between the pixels.\n",
            "[24:15.780 --> 24:18.860]  But another common choice is the L2 or Euclidean distance,\n",
            "[24:18.860 --> 24:21.740]  where you take the square root of the sum of the squares\n",
            "[24:21.740 --> 24:24.100]  and take this as your distance.\n",
            "[24:24.100 --> 24:26.620]  These, choosing different distance metrics\n",
            "[24:26.620 --> 24:29.220]  actually is a pretty interesting topic\n",
            "[24:29.220 --> 24:31.020]  because different distance metrics make different\n",
            "[24:31.020 --> 24:33.940]  assumptions about the underlying geometry or topology\n",
            "[24:33.940 --> 24:35.860]  that you expect in the space.\n",
            "[24:35.860 --> 24:39.180]  So this L1 distance, underneath this,\n",
            "[24:39.180 --> 24:42.140]  this is actually a circle according to the L1 distance,\n",
            "[24:42.140 --> 24:45.540]  and it forms this square shape thing around the origin.\n",
            "[24:45.540 --> 24:48.100]  Where each of the points on this, on the square,\n",
            "[24:48.100 --> 24:51.140]  is equidistant from the origin according to L1.\n",
            "[24:51.140 --> 24:53.260]  Whereas with the L2 or Euclidean distance,\n",
            "[24:53.260 --> 24:55.460]  then this circle is a familiar circle,\n",
            "[24:55.460 --> 24:57.420]  it looks like what you'd expect.\n",
            "[24:57.420 --> 24:58.900]  So one interesting thing to point out\n",
            "[24:58.900 --> 25:01.020]  between these two metrics in particular\n",
            "[25:01.020 --> 25:03.300]  is that the L1 distance depends\n",
            "[25:03.300 --> 25:05.180]  on your choice of coordinate system.\n",
            "[25:05.180 --> 25:07.380]  So if you were to rotate the coordinate frame,\n",
            "[25:07.380 --> 25:09.140]  that would actually change the L1 distance\n",
            "[25:09.140 --> 25:10.260]  between the points.\n",
            "[25:10.260 --> 25:13.620]  Whereas changing the coordinate frame in the L2 distance\n",
            "[25:13.620 --> 25:15.220]  doesn't matter, it's the same thing\n",
            "[25:15.220 --> 25:17.340]  no matter what your coordinate frame is.\n",
            "[25:17.500 --> 25:20.460]  So maybe if your input features,\n",
            "[25:20.460 --> 25:22.900]  if the individual entries in your vector\n",
            "[25:22.900 --> 25:25.140]  have some important meaning for your task,\n",
            "[25:25.140 --> 25:28.460]  then maybe somehow L1 might be a more natural fit.\n",
            "[25:28.460 --> 25:30.740]  But if it's just a generic vector in some space\n",
            "[25:30.740 --> 25:32.900]  and you don't know which of the different elements,\n",
            "[25:32.900 --> 25:34.300]  you don't know what they actually mean,\n",
            "[25:34.300 --> 25:36.300]  then maybe L2 is slightly more natural.\n",
            "[25:37.580 --> 25:40.180]  And another point here is that we can actually,\n",
            "[25:40.180 --> 25:41.980]  by using different distance metrics,\n",
            "[25:41.980 --> 25:44.700]  we can actually generalize the k-nearest neighbor classifier\n",
            "[25:44.700 --> 25:46.500]  to many, many different types of data,\n",
            "[25:46.540 --> 25:48.540]  not just vectors, not just images.\n",
            "[25:48.540 --> 25:50.620]  So for example, imagine you wanted\n",
            "[25:50.620 --> 25:52.500]  to classify pieces of text.\n",
            "[25:52.500 --> 25:54.260]  Then the only thing you need to do\n",
            "[25:54.260 --> 25:56.660]  to use k-nearest neighbors is to specify\n",
            "[25:56.660 --> 26:00.420]  some distance function that can measure distances\n",
            "[26:00.420 --> 26:02.660]  between maybe two paragraphs or two sentences\n",
            "[26:02.660 --> 26:04.020]  or something like that.\n",
            "[26:04.020 --> 26:07.020]  So simply by specifying different distance metrics,\n",
            "[26:07.020 --> 26:09.780]  we can actually apply this algorithm very generally\n",
            "[26:09.780 --> 26:11.740]  to basically any type of data.\n",
            "[26:11.740 --> 26:13.020]  So it's actually a very,\n",
            "[26:13.020 --> 26:14.900]  even though it's a kind of simple algorithm,\n",
            "[26:14.900 --> 26:17.340]  in general, it's a very good thing to try first\n",
            "[26:17.340 --> 26:19.300]  when you're looking at a new problem.\n",
            "[26:21.940 --> 26:24.180]  So then it's also kind of interesting to think about\n",
            "[26:24.180 --> 26:26.020]  what is actually happening geometrically\n",
            "[26:26.020 --> 26:28.540]  if we choose different distance metrics.\n",
            "[26:28.540 --> 26:31.500]  So here we see the same set of points on the left\n",
            "[26:31.500 --> 26:33.900]  using the L1 or Manhattan distance,\n",
            "[26:33.900 --> 26:35.220]  and then on the right,\n",
            "[26:35.220 --> 26:38.340]  using the familiar L2 or Euclidean distance.\n",
            "[26:38.340 --> 26:41.020]  And you can see that the shapes of these decision boundaries\n",
            "[26:41.020 --> 26:44.300]  actually change quite a bit between the two metrics.\n",
            "[26:44.300 --> 26:46.220]  So when you're looking at L1,\n",
            "[26:46.220 --> 26:48.100]  these decision boundaries tend to follow\n",
            "[26:48.100 --> 26:49.580]  the coordinate axes.\n",
            "[26:49.580 --> 26:51.980]  And this is, again, because the L1 actually depends\n",
            "[26:51.980 --> 26:53.660]  on our choice of coordinate system,\n",
            "[26:53.660 --> 26:56.140]  where the L2 sort of doesn't really care\n",
            "[26:56.140 --> 26:58.020]  about the coordinate axes, it just puts the boundaries\n",
            "[26:58.020 --> 27:00.300]  where they sort of should fall naturally.\n",
            "[27:02.420 --> 27:06.780]  So actually, my confession is that each of these examples\n",
            "[27:06.780 --> 27:08.460]  that I've shown you is actually from\n",
            "[27:08.460 --> 27:10.540]  this interactive web demo that I built\n",
            "[27:10.540 --> 27:13.340]  where you can go and play with this k-nearest neighbor\n",
            "[27:13.380 --> 27:14.900]  classifier on your own.\n",
            "[27:14.900 --> 27:18.100]  And this is really hard to work on a projector screen.\n",
            "[27:18.100 --> 27:21.900]  So maybe we'll do that on your own time.\n",
            "[27:26.940 --> 27:28.460]  So let's just go back to here.\n",
            "[27:33.140 --> 27:34.700]  Man, this is kind of embarrassing.\n",
            "[27:43.340 --> 27:44.180]  Okay.\n",
            "[28:07.700 --> 28:10.300]  Okay, that was way more trouble than it was worth.\n",
            "[28:10.300 --> 28:13.140]  So let's skip this, but I encourage you\n",
            "[28:13.820 --> 28:15.020]  to go play with this in your browser.\n",
            "[28:15.020 --> 28:17.580]  It's actually pretty fun and kind of nice\n",
            "[28:17.580 --> 28:20.500]  to build intuition about how the decision boundary changes\n",
            "[28:20.500 --> 28:24.740]  as you change the k and change your distance metric\n",
            "[28:24.740 --> 28:26.260]  and all those sorts of things.\n",
            "[28:31.780 --> 28:33.620]  Okay, so then the question is,\n",
            "[28:33.620 --> 28:35.380]  once you're actually trying to use this algorithm\n",
            "[28:35.380 --> 28:38.340]  in practice, there are several choices you need to make.\n",
            "[28:38.340 --> 28:40.620]  We talked about choosing different values of k.\n",
            "[28:40.620 --> 28:42.820]  We talked about choosing different distance metrics.\n",
            "[28:42.820 --> 28:45.380]  The question becomes, how do you actually make these\n",
            "[28:45.380 --> 28:48.300]  choices for your problem and for your data?\n",
            "[28:48.300 --> 28:52.740]  So these choices of things like k and the distance metric\n",
            "[28:52.740 --> 28:56.580]  we call hyperparameters because they are not necessarily\n",
            "[28:56.580 --> 28:58.300]  learned from the training data.\n",
            "[28:58.300 --> 29:00.540]  Instead, these are choices about your algorithm\n",
            "[29:00.540 --> 29:04.300]  that you make ahead of time and there's no way\n",
            "[29:04.300 --> 29:06.620]  to learn them directly from the data.\n",
            "[29:06.620 --> 29:11.180]  So the question is, how do you set these things in practice?\n",
            "[29:11.180 --> 29:13.500]  And they turn out to be very problem dependent\n",
            "[29:13.500 --> 29:16.380]  and the simple thing that most people do is simply\n",
            "[29:16.380 --> 29:18.980]  try different values of hyperparameters for your data\n",
            "[29:18.980 --> 29:21.940]  and for your problem and figure out which one works best.\n",
            "[29:21.940 --> 29:22.780]  There's a question?\n",
            "[29:30.420 --> 29:33.500]  So the question is where L1 distance might be preferable\n",
            "[29:33.500 --> 29:35.500]  to using L2 distance.\n",
            "[29:35.500 --> 29:37.820]  I think it's mainly problem dependent.\n",
            "[29:37.820 --> 29:40.300]  It's sort of difficult to say in which cases\n",
            "[29:40.340 --> 29:42.580]  you think one might be better than the other.\n",
            "[29:42.580 --> 29:45.820]  But I think that because L1 has this sort of coordinate\n",
            "[29:45.820 --> 29:50.180]  dependency, it actually depends on the coordinate system\n",
            "[29:50.180 --> 29:51.460]  of your data.\n",
            "[29:51.460 --> 29:54.260]  If you sort of know that you have a vector and maybe\n",
            "[29:54.260 --> 29:56.460]  the individual elements of the vector have meaning,\n",
            "[29:56.460 --> 29:59.900]  like maybe you're classifying employees for some reason\n",
            "[29:59.900 --> 30:01.780]  and then the different elements of that vector correspond\n",
            "[30:01.780 --> 30:05.100]  to different features or aspects of an employee\n",
            "[30:05.100 --> 30:07.460]  like their salary or the number of years they've been\n",
            "[30:07.460 --> 30:09.740]  working at the company or something like that.\n",
            "[30:10.220 --> 30:11.980]  I think when your individual elements actually have\n",
            "[30:11.980 --> 30:14.860]  some meaning is where I think maybe using L1 might make\n",
            "[30:14.860 --> 30:16.700]  a little bit more sense.\n",
            "[30:16.700 --> 30:18.580]  But in general, again, this is a hyperparameter\n",
            "[30:18.580 --> 30:20.860]  and it really depends on your problem and your data.\n",
            "[30:20.860 --> 30:23.180]  So the best answer is just to try them both\n",
            "[30:23.180 --> 30:24.460]  and see what works better.\n",
            "[30:27.500 --> 30:30.540]  So then the question, even this idea of trying out\n",
            "[30:30.540 --> 30:32.380]  different values of hyperparameters and seeing\n",
            "[30:32.380 --> 30:35.140]  what works best, there are many different choices here.\n",
            "[30:35.140 --> 30:37.420]  What exactly does it mean to try hyperparameters\n",
            "[30:37.420 --> 30:39.140]  and see what works best?\n",
            "[30:39.540 --> 30:42.460]  Well, the first idea you might think of is simply\n",
            "[30:42.460 --> 30:44.740]  choosing the hyperparameters that give you the best\n",
            "[30:44.740 --> 30:47.820]  accuracy or best performance on your training data.\n",
            "[30:48.860 --> 30:51.060]  This is actually a really terrible idea.\n",
            "[30:51.060 --> 30:52.340]  You should never do this.\n",
            "[30:53.220 --> 30:55.460]  And in the concrete case of the nearest neighbor\n",
            "[30:55.460 --> 30:58.660]  classifier, for example, if we set k equals one,\n",
            "[30:58.660 --> 31:01.420]  we will always classify the training data perfectly.\n",
            "[31:02.340 --> 31:05.740]  So if we use this strategy, we'll always pick k equals one,\n",
            "[31:05.740 --> 31:08.540]  but as we saw from the examples earlier and in practice,\n",
            "[31:08.940 --> 31:11.580]  it seems that the setting k equals to larger values\n",
            "[31:11.580 --> 31:14.460]  might cause us to misclassify some of the training data,\n",
            "[31:14.460 --> 31:16.820]  but in fact lead to better performance on points\n",
            "[31:16.820 --> 31:18.780]  that were not in the training data.\n",
            "[31:18.780 --> 31:21.060]  And ultimately in machine learning, we don't care\n",
            "[31:21.060 --> 31:22.540]  about fitting the training data.\n",
            "[31:22.540 --> 31:25.740]  We really care about how our classifier or how our method\n",
            "[31:25.740 --> 31:28.220]  will perform on unseen data after training.\n",
            "[31:28.220 --> 31:31.580]  So this is a terrible idea, don't do this.\n",
            "[31:31.580 --> 31:34.500]  So another idea that you might think of is maybe\n",
            "[31:34.500 --> 31:36.700]  we'll take our full data set and we'll split it\n",
            "[31:36.700 --> 31:39.380]  into some training data and some test data.\n",
            "[31:39.380 --> 31:43.500]  And now I'll try training my algorithm with different\n",
            "[31:43.500 --> 31:45.660]  choices of hyperparameters on the training data,\n",
            "[31:45.660 --> 31:48.540]  and then I'll go and apply that trained classifier\n",
            "[31:48.540 --> 31:51.220]  on the test data, and now I will pick the set\n",
            "[31:51.220 --> 31:53.620]  of hyperparameters that cause me to perform best\n",
            "[31:53.620 --> 31:54.540]  on the test data.\n",
            "[31:55.580 --> 31:58.500]  This seems like maybe a more reasonable strategy,\n",
            "[31:58.500 --> 32:00.580]  but in fact this is also a terrible idea\n",
            "[32:00.580 --> 32:02.100]  and you should never do this.\n",
            "[32:02.100 --> 32:04.780]  Because again, the point of machine learning systems\n",
            "[32:04.780 --> 32:07.580]  is that we want to know how our algorithm will,\n",
            "[32:07.580 --> 32:10.380]  so the point of the test set is to give us some estimate\n",
            "[32:10.380 --> 32:13.140]  of how our method will do on unseen data\n",
            "[32:13.140 --> 32:15.460]  that's sort of coming out from the wild.\n",
            "[32:15.460 --> 32:18.780]  And if we use this strategy of training many different\n",
            "[32:18.780 --> 32:20.860]  algorithms with different hyperparameters\n",
            "[32:20.860 --> 32:23.380]  and then selecting the one which does the best\n",
            "[32:23.380 --> 32:26.980]  on the test data, then it's possible that we may have\n",
            "[32:26.980 --> 32:29.140]  just picked the right set of hyperparameters\n",
            "[32:29.140 --> 32:31.380]  that caused our algorithm to work quite well\n",
            "[32:31.380 --> 32:33.940]  on this testing set, but now our performance\n",
            "[32:33.940 --> 32:36.500]  on this test set will no longer be representative\n",
            "[32:36.500 --> 32:39.220]  of our performance on new unseen data.\n",
            "[32:39.220 --> 32:42.460]  So again, you should not do this, this is a bad idea,\n",
            "[32:42.460 --> 32:44.220]  you'll get in trouble if you do this.\n",
            "[32:45.580 --> 32:48.060]  What is much more common is to actually split your data\n",
            "[32:48.060 --> 32:50.020]  into three different sets.\n",
            "[32:50.020 --> 32:54.100]  You'll choose, you'll partition most of your data\n",
            "[32:54.100 --> 32:56.020]  into a training set, and then you'll create\n",
            "[32:56.020 --> 32:58.260]  a validation set and a test set.\n",
            "[32:58.260 --> 33:01.740]  And now what we typically do is go and train algorithm,\n",
            "[33:01.740 --> 33:03.660]  train our algorithm with many different choices\n",
            "[33:04.260 --> 33:06.140]  of hyperparameters on the training set,\n",
            "[33:06.140 --> 33:09.100]  evaluate on the validation set, and now pick the set\n",
            "[33:09.100 --> 33:11.060]  of hyperparameters which performs best\n",
            "[33:11.060 --> 33:12.540]  on the validation set.\n",
            "[33:12.540 --> 33:14.620]  And now after you've done all your development,\n",
            "[33:14.620 --> 33:15.900]  after you've done all your debugging,\n",
            "[33:15.900 --> 33:19.500]  after you've done everything, then you take that best\n",
            "[33:19.500 --> 33:22.700]  performing classifier on the validation set\n",
            "[33:22.700 --> 33:24.300]  and run it once on the test set.\n",
            "[33:24.300 --> 33:25.980]  And now that's the number that goes into your paper,\n",
            "[33:25.980 --> 33:28.180]  that's the number that goes into your report,\n",
            "[33:28.180 --> 33:30.340]  that's the number that actually is telling you\n",
            "[33:30.340 --> 33:33.100]  how your algorithm is doing on unseen data.\n",
            "[33:33.420 --> 33:35.500]  And this is actually really, really important\n",
            "[33:35.500 --> 33:37.180]  that you keep a very strict separation\n",
            "[33:37.180 --> 33:39.740]  between the validation data and the test data.\n",
            "[33:39.740 --> 33:42.300]  So for example, when we're working on research papers,\n",
            "[33:42.300 --> 33:44.340]  we typically only touch the test set\n",
            "[33:44.340 --> 33:48.020]  at the very last minute, so when I'm writing papers,\n",
            "[33:48.020 --> 33:50.420]  I tend to only touch the test set for my problem\n",
            "[33:50.420 --> 33:52.700]  in maybe the week before the deadline or so\n",
            "[33:52.700 --> 33:56.260]  to really ensure that we're not being dishonest here\n",
            "[33:56.260 --> 33:58.780]  and we're not reporting a number which is unfair.\n",
            "[33:58.780 --> 34:01.000]  So this is actually super, super important,\n",
            "[34:01.000 --> 34:03.120]  and you want to make sure to keep your test data\n",
            "[34:03.120 --> 34:04.240]  quite under control.\n",
            "[34:07.240 --> 34:09.600]  So another strategy for setting hyperparameters\n",
            "[34:09.600 --> 34:13.320]  is called cross-validation, and this is used\n",
            "[34:13.320 --> 34:15.880]  a little bit more commonly for small data sets,\n",
            "[34:15.880 --> 34:18.160]  not used so much in deep learning.\n",
            "[34:18.160 --> 34:21.040]  So here the idea is that we're gonna take our test data,\n",
            "[34:21.040 --> 34:23.400]  or we're gonna take our data set, as usual,\n",
            "[34:23.400 --> 34:26.360]  hold out some test set to use at the very end,\n",
            "[34:26.360 --> 34:27.840]  and now for the rest of the data,\n",
            "[34:27.840 --> 34:30.200]  rather than splitting it into a single training\n",
            "[34:30.200 --> 34:33.840]  and validation partition, instead we can split\n",
            "[34:33.840 --> 34:36.360]  our training data into many different folds,\n",
            "[34:36.360 --> 34:38.900]  and now in this way, we kind of cycle through\n",
            "[34:38.900 --> 34:42.220]  choosing which fold is going to be the validation set.\n",
            "[34:42.220 --> 34:45.360]  So now in this example, we're using five-fold\n",
            "[34:45.360 --> 34:47.920]  cross-validation, so you would train your algorithm\n",
            "[34:47.920 --> 34:50.840]  with one set of hyperparameters on the first four folds,\n",
            "[34:50.840 --> 34:52.840]  evaluate the performance on fold four,\n",
            "[34:52.840 --> 34:55.340]  and now go and retrain your algorithm on folds one,\n",
            "[34:55.340 --> 34:58.620]  two, three, and five, evaluate on fold four,\n",
            "[34:58.760 --> 35:01.220]  and sort of cycle through all the different folds.\n",
            "[35:01.220 --> 35:04.900]  And when you do it this way, you get much higher confidence\n",
            "[35:04.900 --> 35:07.260]  about which hyperparameters are going to perform\n",
            "[35:07.260 --> 35:10.760]  more robustly, so this is kind of the gold standard to use,\n",
            "[35:10.760 --> 35:12.700]  but in practice, in deep learning,\n",
            "[35:12.700 --> 35:14.260]  when we're training large models,\n",
            "[35:14.260 --> 35:17.000]  and training is very computationally expensive,\n",
            "[35:17.000 --> 35:19.580]  this doesn't get used too much in practice.\n",
            "[35:19.580 --> 35:20.400]  Question?\n",
            "[35:29.620 --> 35:32.700]  Yeah, so the question is a little bit more concretely,\n",
            "[35:32.700 --> 35:34.260]  what's the difference between the training\n",
            "[35:34.260 --> 35:35.980]  and the validation set?\n",
            "[35:35.980 --> 35:40.140]  So, if you think about the k-nearest neighbor classifier,\n",
            "[35:41.260 --> 35:45.460]  then the training set is this set of images with labels\n",
            "[35:45.460 --> 35:48.660]  where we memorize the labels, and now to classify an image,\n",
            "[35:48.660 --> 35:50.220]  we're going to take the image and compare it\n",
            "[35:50.220 --> 35:52.660]  to each element in the training data,\n",
            "[35:52.660 --> 35:56.820]  and then transfer the label from the nearest training point.\n",
            "[35:59.100 --> 36:00.900]  So now what we're going to do with the validation,\n",
            "[36:00.900 --> 36:02.940]  so now our algorithm will memorize everything\n",
            "[36:02.940 --> 36:05.500]  in the training set, and now it will take each element\n",
            "[36:05.500 --> 36:07.940]  of the validation set and compare it to each element\n",
            "[36:07.940 --> 36:11.940]  in the training data, and then use this to determine\n",
            "[36:11.940 --> 36:14.200]  what is the accuracy of our classifier\n",
            "[36:14.200 --> 36:17.580]  when it's applied on the validation set.\n",
            "[36:17.580 --> 36:19.580]  So this is kind of the distinction between training\n",
            "[36:19.580 --> 36:22.380]  and validation, where your algorithm is able to see\n",
            "[36:22.380 --> 36:26.520]  the labels of the training set, but for the validation set,\n",
            "[36:26.520 --> 36:28.380]  your algorithm doesn't have direct access\n",
            "[36:29.020 --> 36:31.460]  to the labels, we only use the labels of the validation set\n",
            "[36:31.460 --> 36:34.580]  to check how well our algorithm is doing.\n",
            "[36:34.580 --> 36:35.500]  A question?\n",
            "[36:44.860 --> 36:48.220]  The question is whether the test set,\n",
            "[36:48.220 --> 36:50.980]  is it possible that the test set might not be representative\n",
            "[36:50.980 --> 36:53.300]  of data out there in the wild?\n",
            "[36:53.300 --> 36:56.500]  And this definitely can be a problem in practice.\n",
            "[36:56.660 --> 36:58.700]  The underlying statistical assumption here\n",
            "[36:58.700 --> 37:01.020]  is that your data are all independently\n",
            "[37:01.020 --> 37:06.020]  and identically distributed, so that all of your data points\n",
            "[37:06.460 --> 37:08.980]  should be sort of drawn from the same underlying\n",
            "[37:08.980 --> 37:11.020]  probability distribution.\n",
            "[37:11.020 --> 37:13.660]  Of course in practice, this might not always be the case,\n",
            "[37:13.660 --> 37:16.340]  and you definitely can run into cases where the test set\n",
            "[37:16.340 --> 37:19.540]  might not actually be, might not be super representative\n",
            "[37:19.540 --> 37:21.660]  of what you see in the wild.\n",
            "[37:21.660 --> 37:24.420]  So this is kind of a problem that data set creators\n",
            "[37:24.460 --> 37:26.380]  and data set curators need to think about,\n",
            "[37:26.380 --> 37:28.460]  but when I'm creating data sets, for example,\n",
            "[37:28.460 --> 37:30.980]  one thing I do is I'll go and collect a whole bunch\n",
            "[37:30.980 --> 37:33.940]  of data all at once, using the exact same methodology\n",
            "[37:33.940 --> 37:36.140]  for collecting the data, and then afterwards,\n",
            "[37:36.140 --> 37:39.540]  you go and partition it randomly between train and test.\n",
            "[37:39.540 --> 37:41.940]  One thing that can screw you up here is maybe\n",
            "[37:41.940 --> 37:43.860]  if you're collecting data over time,\n",
            "[37:43.860 --> 37:46.220]  and you make the earlier data that you collect first\n",
            "[37:46.220 --> 37:48.420]  be the training data, and the later data that you collect\n",
            "[37:48.420 --> 37:50.340]  be the test data, then you actually might run into\n",
            "[37:50.340 --> 37:52.620]  this kind of shift that could cause problems.\n",
            "[37:52.660 --> 37:55.580]  But as long as this partition is random among your entire\n",
            "[37:55.580 --> 37:58.540]  set of data points, then that's how we sort of try\n",
            "[37:58.540 --> 38:00.420]  to alleviate this problem in practice.\n",
            "[38:04.980 --> 38:08.380]  So then once you've gone through this cross-validation\n",
            "[38:08.380 --> 38:11.060]  procedure, then you end up with graphs that look\n",
            "[38:11.060 --> 38:12.220]  something like this.\n",
            "[38:12.220 --> 38:15.540]  So here on the x-axis, we are showing the value of k\n",
            "[38:15.540 --> 38:18.380]  for a k-nearest neighbor classifier on some problem,\n",
            "[38:18.380 --> 38:22.380]  and now on the y-axis, we are showing what is the accuracy\n",
            "[38:22.540 --> 38:26.940]  of our classifier on some data set for different values\n",
            "[38:26.940 --> 38:29.660]  of k, and you can see that in this case, we've done\n",
            "[38:29.660 --> 38:32.660]  five-fold cross-validation over the data,\n",
            "[38:32.660 --> 38:35.620]  so for each value of k, we have five different examples\n",
            "[38:35.620 --> 38:39.420]  of how well this algorithm is doing.\n",
            "[38:39.420 --> 38:41.380]  And actually, sort of going back to the question\n",
            "[38:41.380 --> 38:44.180]  about having some test sets that are better or worse\n",
            "[38:44.180 --> 38:47.020]  for your algorithm, this is sort of,\n",
            "[38:47.020 --> 38:49.400]  using k-fold cross-validation is maybe one way\n",
            "[38:49.400 --> 38:51.300]  to help quantify that a little bit.\n",
            "[38:51.380 --> 38:54.580]  And in that, we can kind of see the variance\n",
            "[38:54.580 --> 38:57.260]  of how this algorithm performs on different\n",
            "[38:57.260 --> 38:59.820]  of the validation folds, and that gives you some sense\n",
            "[38:59.820 --> 39:02.460]  of not just what is the best, but also what is the\n",
            "[39:02.460 --> 39:04.180]  distribution of that performance.\n",
            "[39:05.300 --> 39:07.460]  So whenever you're training machine learning models,\n",
            "[39:07.460 --> 39:09.780]  you end up making plots like this that show you\n",
            "[39:09.780 --> 39:12.220]  what is your accuracy or your performance as a function\n",
            "[39:12.220 --> 39:14.900]  of your hyperparameters, and then you want to go and pick\n",
            "[39:14.900 --> 39:17.380]  the model or the set of hyperparameters at the end\n",
            "[39:17.380 --> 39:20.940]  of the day that performs the best on the validation set.\n",
            "[39:21.460 --> 39:23.900]  So here, we see that maybe about k equals seven\n",
            "[39:23.900 --> 39:25.940]  probably works about best for this problem.\n",
            "[39:28.140 --> 39:33.060]  So one thing that is, so k-nearest neighbor classifiers\n",
            "[39:33.060 --> 39:36.060]  on images are actually almost never used in practice,\n",
            "[39:36.060 --> 39:38.540]  because they're just, with all of these problems\n",
            "[39:38.540 --> 39:39.740]  that we've talked about.\n",
            "[39:39.740 --> 39:42.700]  So one problem is that it's very slow at test time,\n",
            "[39:42.700 --> 39:44.380]  which is kind of the reverse of what we want,\n",
            "[39:44.380 --> 39:45.900]  which we talked about earlier.\n",
            "[39:45.900 --> 39:49.020]  Another problem is that these things like Euclidean distance\n",
            "[39:49.060 --> 39:53.220]  are really not a very good way to measure distances\n",
            "[39:53.220 --> 39:54.700]  between images.\n",
            "[39:54.700 --> 39:57.660]  These sort of vectorial distance functions do not\n",
            "[39:57.660 --> 40:00.300]  correspond very well to perceptual similarity between\n",
            "[40:00.300 --> 40:04.180]  images, how you perceive differences between images.\n",
            "[40:04.180 --> 40:07.660]  So in this example, we've constructed, there's this image\n",
            "[40:07.660 --> 40:10.180]  on the left of a girl, and then three different distorted\n",
            "[40:10.180 --> 40:13.140]  images on the right, where we've blocked out her mouth,\n",
            "[40:13.140 --> 40:16.220]  we've actually shifted down by a couple pixels,\n",
            "[40:16.220 --> 40:18.380]  or tinted the entire image blue.\n",
            "[40:18.420 --> 40:20.980]  And actually, if you compute the Euclidean distance\n",
            "[40:20.980 --> 40:23.180]  between the original and the boxed, the original\n",
            "[40:23.180 --> 40:25.500]  and the shuffled, and the original and the tinted,\n",
            "[40:25.500 --> 40:28.620]  they all have the same L2 distance, which is maybe\n",
            "[40:28.620 --> 40:31.780]  not so good, because it sort of gives you the sense\n",
            "[40:31.780 --> 40:34.980]  that the L2 distance is really not doing a very good job\n",
            "[40:34.980 --> 40:37.820]  at capturing these perceptual differences between images.\n",
            "[40:40.580 --> 40:43.020]  Another sort of problem with the k-nearest neighbor\n",
            "[40:43.020 --> 40:45.180]  classifier has to do with something we call\n",
            "[40:45.180 --> 40:46.820]  the curse of dimensionality.\n",
            "[40:46.860 --> 40:49.860]  So if you recall back, this viewpoint we had of the\n",
            "[40:49.860 --> 40:52.460]  k-nearest neighbor classifier, it's sort of dropping paint\n",
            "[40:52.460 --> 40:55.460]  around each of the training data points, and using that\n",
            "[40:55.460 --> 40:57.420]  to sort of partition the space.\n",
            "[40:57.420 --> 40:59.700]  So that means that if we expect the k-nearest neighbor\n",
            "[40:59.700 --> 41:02.860]  classifier to work well, we kind of need our train examples\n",
            "[41:02.860 --> 41:05.780]  to cover the space quite densely.\n",
            "[41:05.780 --> 41:10.460]  Otherwise, our nearest neighbors could actually be\n",
            "[41:10.460 --> 41:13.020]  quite far away, and might not actually be very similar\n",
            "[41:13.020 --> 41:15.420]  to our testing points.\n",
            "[41:15.460 --> 41:18.420]  So, and the problem is that actually densely covering\n",
            "[41:18.420 --> 41:21.500]  the space means that we need a number of training examples,\n",
            "[41:21.500 --> 41:24.820]  which is exponential in the dimension of the problem.\n",
            "[41:24.820 --> 41:27.460]  So this is very bad, exponential growth is always bad,\n",
            "[41:27.460 --> 41:29.980]  and you're never gonna get enough, basically you're never\n",
            "[41:29.980 --> 41:32.820]  gonna get enough images to densely cover this space\n",
            "[41:32.820 --> 41:35.580]  of pixels in this high-dimensional space.\n",
            "[41:35.580 --> 41:37.700]  So that's maybe another thing to keep in mind\n",
            "[41:37.700 --> 41:41.220]  when you're thinking about using k-nearest neighbor.\n",
            "[41:41.220 --> 41:43.340]  So kind of the summary is that we're using k-nearest\n",
            "[41:43.380 --> 41:45.900]  neighbor to introduce this idea of image classification.\n",
            "[41:45.900 --> 41:47.660]  We have a training set of images and labels,\n",
            "[41:47.660 --> 41:50.300]  and then we use that to predict these labels\n",
            "[41:50.300 --> 41:51.460]  on the test set.\n",
            "[41:51.460 --> 41:52.300]  Question?\n",
            "[41:54.420 --> 41:56.740]  Oh, sorry, the question is what was going on\n",
            "[41:56.740 --> 41:59.460]  with this picture, what are the green and blue dots?\n",
            "[41:59.460 --> 42:03.500]  So here, this was maybe, we have some training samples\n",
            "[42:03.500 --> 42:06.860]  which are represented by points, and the color of the dot\n",
            "[42:06.860 --> 42:11.100]  maybe represents the category of this training sample.\n",
            "[42:11.140 --> 42:13.860]  So if we're in one dimension, then you maybe only need\n",
            "[42:13.860 --> 42:17.660]  four training samples to kind of densely cover the space.\n",
            "[42:17.660 --> 42:20.460]  But if we move to two dimensions, then we now need\n",
            "[42:20.460 --> 42:23.340]  four times four is 16 training examples to densely\n",
            "[42:23.340 --> 42:25.740]  cover this space.\n",
            "[42:25.740 --> 42:28.780]  And if we move to three, four, five, many more dimensions,\n",
            "[42:28.780 --> 42:31.020]  the number of training examples that we need to densely\n",
            "[42:31.020 --> 42:35.180]  cover the space grows exponentially with the dimension.\n",
            "[42:35.180 --> 42:36.860]  So this is kind of giving you this sense that maybe\n",
            "[42:36.860 --> 42:39.260]  in two dimensions, we might have this kind of funny\n",
            "[42:39.300 --> 42:42.060]  curved shape, or you might have sort of arbitrary\n",
            "[42:42.060 --> 42:46.460]  manifolds of labels in different dimensional spaces.\n",
            "[42:46.460 --> 42:48.700]  And the only way that the, because the k-nearest neighbor\n",
            "[42:48.700 --> 42:51.500]  algorithm doesn't really make any assumptions about\n",
            "[42:51.500 --> 42:53.820]  these underlying manifolds, the only way it can perform\n",
            "[42:53.820 --> 42:57.340]  properly is if it has quite a dense sample of training\n",
            "[42:57.340 --> 42:59.020]  points to work with.\n",
            "[43:01.580 --> 43:04.780]  So this is kind of the overview of k-nearest neighbors,\n",
            "[43:04.780 --> 43:06.940]  and you'll get a chance to actually implement this\n",
            "[43:06.940 --> 43:10.940]  and try it out on images in the first assignment.\n",
            "[43:10.940 --> 43:12.860]  So if there's any last minute questions about k-NN,\n",
            "[43:12.860 --> 43:15.980]  I'm going to move on to the next topic.\n",
            "[43:15.980 --> 43:16.820]  Question?\n",
            "[43:21.020 --> 43:22.860]  Sorry, say that again.\n",
            "[43:28.140 --> 43:30.620]  Yeah, so the question is why do these images have the same\n",
            "[43:30.620 --> 43:33.980]  L2 distance, and the answer is that I carefully constructed\n",
            "[43:33.980 --> 43:36.060]  them to have the same L2 distance.\n",
            "[43:36.780 --> 43:40.940]  So, but it's just giving you this sense that the L2 distance\n",
            "[43:40.940 --> 43:44.300]  is not a very good measure of similarity between images,\n",
            "[43:44.300 --> 43:46.860]  and these images are actually all different from each other\n",
            "[43:46.860 --> 43:49.500]  in quite disparate ways.\n",
            "[43:50.700 --> 43:54.860]  But if you're using k-NN, then the only thing you have\n",
            "[43:54.860 --> 43:58.460]  to measure distance between images is this single distance\n",
            "[43:58.460 --> 44:02.140]  metric, and this kind of gives you an example where that\n",
            "[44:02.140 --> 44:05.340]  distance metric is actually not capturing the same distance\n",
            "[44:05.340 --> 44:07.900]  or actually not capturing the full description of distance\n",
            "[44:07.900 --> 44:09.900]  or difference between images.\n",
            "[44:09.900 --> 44:12.300]  But yes, in this case, I just sort of carefully constructed\n",
            "[44:12.300 --> 44:16.380]  these translations and these offsets to match exactly.\n",
            "[44:16.380 --> 44:17.220]  Question?\n",
            "[44:29.580 --> 44:32.540]  So the question is, maybe this is actually good,\n",
            "[44:32.540 --> 44:35.100]  because all of these things are actually having\n",
            "[44:35.100 --> 44:37.580]  the same distance to the image.\n",
            "[44:37.580 --> 44:39.660]  That's maybe true for this example, but I think you could\n",
            "[44:39.660 --> 44:42.380]  also construct examples where maybe we have two original\n",
            "[44:42.380 --> 44:44.500]  images, and then by putting the boxes in the right places\n",
            "[44:44.500 --> 44:47.620]  or tinting them, we could cause it to be nearer to pretty\n",
            "[44:47.620 --> 44:49.340]  much anything that you want, right?\n",
            "[44:49.340 --> 44:51.900]  Because in this example, we can kind of do arbitrary\n",
            "[44:51.900 --> 44:54.580]  shifting and tinting to kind of change these distances\n",
            "[44:54.580 --> 44:57.660]  nearly arbitrarily without changing the perceptual nature\n",
            "[44:57.660 --> 44:58.620]  of these images.\n",
            "[44:58.620 --> 45:00.980]  So I think that this could actually screw you up if you have\n",
            "[45:00.980 --> 45:02.940]  many different original images.\n",
            "[45:03.180 --> 45:04.020]  Question?\n",
            "[45:15.180 --> 45:17.180]  The question is whether or not it's common in real world\n",
            "[45:17.180 --> 45:20.780]  cases to go back and retrain on the entire data set\n",
            "[45:20.780 --> 45:24.220]  once you've found those best hyperparameters.\n",
            "[45:24.220 --> 45:28.620]  So people do sometimes do this in practice,\n",
            "[45:28.620 --> 45:31.020]  but it's somewhat a matter of taste.\n",
            "[45:31.060 --> 45:32.980]  If you're really rushing for that deadline and you really\n",
            "[45:32.980 --> 45:35.580]  gotta get this model out the door, then if it takes a long\n",
            "[45:35.580 --> 45:38.220]  time to retrain the model on the whole data set,\n",
            "[45:38.220 --> 45:39.900]  then maybe you won't do it.\n",
            "[45:39.900 --> 45:42.180]  But if you have a little bit more time to spare and a little\n",
            "[45:42.180 --> 45:44.620]  bit more compute to spare, and you wanna squeeze out maybe\n",
            "[45:44.620 --> 45:47.700]  that extra 1% of performance, then that is a trick\n",
            "[45:47.700 --> 45:48.540]  you can use.\n",
            "[45:52.220 --> 45:55.060]  So moving on from, so we kind of saw the k-nearest neighbor\n",
            "[45:55.060 --> 45:57.300]  has a lot of the nice properties of machine learning\n",
            "[45:57.300 --> 46:00.540]  algorithms, but in practice, it's not so bad.\n",
            "[46:00.580 --> 46:04.740]  It's not so great, and really not used very much in images.\n",
            "[46:05.980 --> 46:08.300]  So the next thing I'd like to talk about is linear\n",
            "[46:08.300 --> 46:11.620]  classification, and linear classification is, again,\n",
            "[46:11.620 --> 46:14.460]  quite a simple learning algorithm, but this will become\n",
            "[46:14.460 --> 46:17.740]  super important and help us build up to whole neural\n",
            "[46:17.740 --> 46:20.500]  networks and whole convolutional networks.\n",
            "[46:20.500 --> 46:23.460]  So one analogy people often talk about when working with\n",
            "[46:23.460 --> 46:26.020]  neural networks is we think of them as being kind of like\n",
            "[46:26.020 --> 46:29.060]  Lego blocks, that you can have different kinds of components\n",
            "[46:29.060 --> 46:31.340]  of neural networks, and you can stick these components\n",
            "[46:31.340 --> 46:34.860]  together to build these large different towers of\n",
            "[46:34.860 --> 46:36.260]  convolutional networks.\n",
            "[46:36.260 --> 46:38.740]  And kind of one of the most basic building blocks that\n",
            "[46:38.740 --> 46:42.380]  we'll see in different types of deep learning applications\n",
            "[46:42.380 --> 46:44.060]  is this linear classifier.\n",
            "[46:44.060 --> 46:46.620]  So I think it's actually really important to have a good\n",
            "[46:46.620 --> 46:48.140]  understanding of what's happening with linear\n",
            "[46:48.140 --> 46:50.380]  classification, because this will end up generalizing\n",
            "[46:50.380 --> 46:53.420]  quite nicely to whole neural networks.\n",
            "[46:53.420 --> 46:56.500]  So another example of kind of this modular nature of neural\n",
            "[46:56.500 --> 46:58.900]  networks comes from some research in our own lab on image\n",
            "[46:58.900 --> 47:01.100]  captioning, just as a little bit of a preview.\n",
            "[47:01.100 --> 47:03.860]  So here the setup is that we want to input an image, and\n",
            "[47:03.860 --> 47:06.940]  then output a descriptive sentence describing the image.\n",
            "[47:06.940 --> 47:10.100]  And the way this kind of works is that we have one\n",
            "[47:10.100 --> 47:12.460]  convolutional neural network that's looking at the image,\n",
            "[47:12.460 --> 47:15.380]  and a recurrent neural network that knows about language,\n",
            "[47:15.380 --> 47:17.460]  and we can kind of just stick these two pieces together\n",
            "[47:17.460 --> 47:19.740]  like Lego blocks and train the whole thing together,\n",
            "[47:19.740 --> 47:22.380]  and end up with a pretty cool system that can do some\n",
            "[47:22.380 --> 47:23.660]  non-trivial things.\n",
            "[47:23.660 --> 47:25.940]  And we'll work through the details of this model as we go\n",
            "[47:25.940 --> 47:28.340]  forward in the class, but this just gives you the sense\n",
            "[47:28.340 --> 47:31.100]  that these deep neural networks are kind of like Legos,\n",
            "[47:31.100 --> 47:34.300]  and this linear classifier is kind of like the most basic\n",
            "[47:34.300 --> 47:37.820]  building block of these giant networks.\n",
            "[47:37.820 --> 47:40.060]  But that's a little bit too exciting for lecture two,\n",
            "[47:40.060 --> 47:43.140]  so we have to go back to CIFAR-10 for the moment.\n",
            "[47:43.140 --> 47:46.460]  So recall that CIFAR-10 has these 50,000 train examples,\n",
            "[47:46.460 --> 47:51.020]  each image is 32 by 32 pixels and three color channels.\n",
            "[47:51.020 --> 47:54.020]  So now we're going to take a bit of, in linear classification\n",
            "[47:54.020 --> 47:56.060]  we're going to take a bit of a different approach from\n",
            "[47:56.060 --> 47:57.460]  k-nearest neighbor.\n",
            "[47:57.580 --> 48:01.740]  So the linear classifier is one of the simplest examples\n",
            "[48:02.740 --> 48:05.500]  of what we call a parametric model.\n",
            "[48:05.500 --> 48:08.580]  So now our parametric model actually has two different\n",
            "[48:08.580 --> 48:11.620]  components, it's going to take in this image,\n",
            "[48:11.620 --> 48:15.780]  maybe of a cat on the left, and this, that we usually write\n",
            "[48:15.780 --> 48:20.300]  as x for our input data, and also a set of parameters\n",
            "[48:20.300 --> 48:24.460]  or weights, which is usually called w, also sometimes theta\n",
            "[48:24.460 --> 48:25.980]  depending on the literature.\n",
            "[48:26.020 --> 48:28.460]  And now we're going to write down some function which takes\n",
            "[48:28.460 --> 48:32.580]  in both the data x and the parameters w, and this will spit\n",
            "[48:32.580 --> 48:37.460]  out now 10 numbers describing what are the scores\n",
            "[48:37.460 --> 48:40.940]  corresponding to each of those 10 categories in CIFAR-10.\n",
            "[48:40.940 --> 48:44.140]  With the interpretation that larger scores, like the larger\n",
            "[48:44.140 --> 48:47.420]  score for cat indicates a larger probability of that input\n",
            "[48:47.420 --> 48:48.260]  x being cat.\n",
            "[48:49.540 --> 48:50.540]  And now a question.\n",
            "[48:55.980 --> 48:59.620]  Oh, so the question is what is the three?\n",
            "[48:59.620 --> 49:03.820]  So the three in this example corresponds to the three\n",
            "[49:03.820 --> 49:06.580]  color channels, red, green, and blue, because we typically\n",
            "[49:06.580 --> 49:09.260]  work on color images, that's nice information that you\n",
            "[49:09.260 --> 49:10.420]  don't want to throw away.\n",
            "[49:11.860 --> 49:14.460]  So what's kind of different about this parametric approach,\n",
            "[49:14.460 --> 49:18.060]  so in the k-nearest neighbor setup there was no parameters,\n",
            "[49:18.060 --> 49:20.820]  instead we just sort of keep around the whole training data,\n",
            "[49:20.820 --> 49:23.420]  the whole training set, and use that at test time.\n",
            "[49:23.420 --> 49:26.180]  But now in a parametric approach we're going to summarize\n",
            "[49:26.180 --> 49:28.420]  our knowledge of the training data and stick all that\n",
            "[49:28.420 --> 49:30.420]  knowledge into these parameters w.\n",
            "[49:30.420 --> 49:33.420]  And now at test time we no longer need the actual training\n",
            "[49:33.420 --> 49:35.940]  data, we can throw it away, we only need these parameters\n",
            "[49:35.940 --> 49:39.420]  w at test time, so this allows our models to now be more\n",
            "[49:39.420 --> 49:42.060]  efficient and actually run on maybe small devices like\n",
            "[49:42.060 --> 49:42.900]  phones.\n",
            "[49:44.020 --> 49:47.620]  So kind of the whole story in deep learning is coming up\n",
            "[49:47.620 --> 49:50.140]  with the right structure for this function f.\n",
            "[49:50.140 --> 49:52.700]  You can imagine writing down different features of the\n",
            "[49:52.700 --> 49:54.420]  system, writing down different functional forms for how\n",
            "[49:54.420 --> 49:57.460]  to combine weights and data in different complex ways,\n",
            "[49:57.460 --> 49:59.940]  and these could correspond to different network\n",
            "[49:59.940 --> 50:00.860]  architectures.\n",
            "[50:01.940 --> 50:04.620]  But the simplest possible example of combining these two\n",
            "[50:04.620 --> 50:06.620]  things is just maybe to multiply them.\n",
            "[50:06.620 --> 50:09.540]  And this is a linear classifier.\n",
            "[50:09.540 --> 50:13.540]  So here our f of x, w is just equal to the w times x.\n",
            "[50:13.540 --> 50:16.500]  Very, probably the simplest equation you can imagine.\n",
            "[50:16.500 --> 50:18.740]  So here if you kind of unpack the dimensions of these\n",
            "[50:18.780 --> 50:22.940]  things, we recall that our image was maybe 32 by 32 by 3\n",
            "[50:22.940 --> 50:27.100]  values, so then this becomes, we're gonna take those values\n",
            "[50:28.460 --> 50:32.460]  and then stretch them out into a long column vector that\n",
            "[50:32.460 --> 50:35.460]  has 3072 by one entries.\n",
            "[50:35.460 --> 50:38.860]  And now we need to end up, we want to end up with 10 class\n",
            "[50:38.860 --> 50:42.540]  scores, we want to end up with 10 numbers for this image,\n",
            "[50:42.540 --> 50:45.060]  giving us the scores for each of the 10 categories,\n",
            "[50:45.060 --> 50:48.700]  which means that now our matrix w needs to be 10 by 32\n",
            "[50:49.700 --> 50:52.060]  by 3072, so that once we multiply these two things out,\n",
            "[50:52.060 --> 50:55.020]  then we'll end up with a single column vector 10 by one,\n",
            "[50:55.020 --> 50:56.540]  giving us our 10 class scores.\n",
            "[50:57.980 --> 51:01.580]  Also sometimes you'll typically see this is we'll often add\n",
            "[51:01.580 --> 51:05.420]  a bias term, which will be a constant vector of 10 elements\n",
            "[51:05.420 --> 51:08.060]  that does not interact with the training data, and instead\n",
            "[51:08.060 --> 51:11.140]  just gives us some sort of data independent preferences\n",
            "[51:11.140 --> 51:12.940]  for some classes over another.\n",
            "[51:12.940 --> 51:15.380]  So you might imagine that if your data set was unbalanced\n",
            "[51:15.380 --> 51:18.180]  and had many more cats than dogs, for example, then the\n",
            "[51:18.700 --> 51:21.060]  bias element corresponding to cat would be higher than\n",
            "[51:21.060 --> 51:21.900]  the other ones.\n",
            "[51:23.540 --> 51:26.900]  So if you kind of think about pictorially what this function\n",
            "[51:26.900 --> 51:30.260]  is doing, we're taking, we have in this figure, we have\n",
            "[51:30.260 --> 51:34.980]  an example on the left of a simple image with just a two\n",
            "[51:34.980 --> 51:37.380]  by two image, so it has four pixels total.\n",
            "[51:37.380 --> 51:40.700]  So the way that the linear classifier works is that we take\n",
            "[51:40.700 --> 51:44.140]  this two by two image, we stretch it out into a four,\n",
            "[51:44.140 --> 51:47.460]  into a column vector with four elements, and now it's\n",
            "[51:47.540 --> 51:50.580]  in this example, we are just restricting to three classes,\n",
            "[51:50.580 --> 51:54.340]  cat, dog, and ship, because you can't fit 10 on a slide.\n",
            "[51:54.340 --> 51:59.340]  And now our weight matrix is going to be four by three,\n",
            "[52:00.060 --> 52:02.420]  so we have four pixels and three classes.\n",
            "[52:02.420 --> 52:06.180]  And now, again, we have a three element bias vector that\n",
            "[52:06.180 --> 52:10.460]  gives us data independent bias terms for each category.\n",
            "[52:10.460 --> 52:13.860]  So now we see that the cat score is gonna be the inner\n",
            "[52:13.980 --> 52:18.980]  product between the pixels of our image and this row in the\n",
            "[52:19.660 --> 52:23.100]  weight matrix added together with this bias term.\n",
            "[52:23.100 --> 52:26.540]  So when you look at it this way, you can kind of understand\n",
            "[52:26.540 --> 52:29.620]  linear classification as almost a template matching\n",
            "[52:29.620 --> 52:33.300]  approach, where each of the rows in this matrix correspond\n",
            "[52:33.300 --> 52:37.620]  to some template of the image, and now the inner product\n",
            "[52:37.620 --> 52:41.620]  or dot product between the row of the matrix and the column\n",
            "[52:41.620 --> 52:44.700]  giving the pixels of the image, computing this dot product\n",
            "[52:44.700 --> 52:47.380]  kind of gives us a similarity between this template for\n",
            "[52:47.380 --> 52:50.460]  the class and the pixels of our image.\n",
            "[52:50.460 --> 52:53.020]  And then this bias just, again, gives you this data\n",
            "[52:53.020 --> 52:57.180]  independent scaling offset to each of the classes.\n",
            "[52:58.620 --> 53:01.540]  So now, from this template matching, if we think about\n",
            "[53:01.540 --> 53:04.340]  linear classification from this viewpoint of template\n",
            "[53:04.340 --> 53:07.140]  matching, we can actually take the rows of that weight\n",
            "[53:07.140 --> 53:10.660]  matrix and unravel them back into images, and actually\n",
            "[53:10.660 --> 53:13.460]  visualize those templates as images, and this gives us\n",
            "[53:13.460 --> 53:16.340]  some sense of what a linear classifier might actually\n",
            "[53:16.340 --> 53:19.060]  be doing to try to understand our data.\n",
            "[53:19.060 --> 53:21.300]  So in this example, we've gone ahead and trained a\n",
            "[53:21.300 --> 53:24.420]  linear classifier on our images, and now on the bottom\n",
            "[53:24.420 --> 53:28.260]  we're visualizing what are those rows in that learned\n",
            "[53:28.260 --> 53:31.060]  weight matrix corresponding to each of the 10 categories\n",
            "[53:31.060 --> 53:33.900]  in CIFAR-10, and in this way, we kind of get a sense\n",
            "[53:33.900 --> 53:35.980]  for what's going on in these images.\n",
            "[53:35.980 --> 53:39.300]  So for example, in the left, on the bottom left, we see\n",
            "[53:39.340 --> 53:42.260]  the template for the plane class kind of consists of this\n",
            "[53:42.260 --> 53:44.980]  blue blob, this kind of blobby thing in the middle,\n",
            "[53:44.980 --> 53:47.740]  and maybe blue in the background, which gives you the sense\n",
            "[53:47.740 --> 53:50.940]  that this linear classifier for plane is maybe looking\n",
            "[53:50.940 --> 53:54.260]  for blue stuff and blobby stuff, and those features are\n",
            "[53:54.260 --> 53:57.620]  going to cause the classifier to like planes more.\n",
            "[53:57.620 --> 54:00.380]  Or if we look at this car example, we kind of see that\n",
            "[54:00.380 --> 54:03.820]  there's a red blobby thing through the middle, and a blue\n",
            "[54:03.820 --> 54:06.020]  blobby thing at the top that maybe is kind of a blurry\n",
            "[54:06.020 --> 54:06.940]  windshield.\n",
            "[54:07.940 --> 54:09.340]  But this is a little bit weird.\n",
            "[54:09.340 --> 54:10.980]  This doesn't really look like a car.\n",
            "[54:10.980 --> 54:13.500]  No individual car actually looks like this.\n",
            "[54:13.500 --> 54:16.060]  So the problem is that the linear classifier is only\n",
            "[54:16.060 --> 54:19.100]  learning one template for each class, so if there's sort\n",
            "[54:19.100 --> 54:21.980]  of variations in how that class might appear, it's trying\n",
            "[54:21.980 --> 54:24.500]  to average out all those different variations, all those\n",
            "[54:24.500 --> 54:27.340]  different appearances, and use just one single template\n",
            "[54:27.340 --> 54:29.460]  to recognize each of those categories.\n",
            "[54:29.460 --> 54:31.900]  We can also see this pretty explicitly in the horse\n",
            "[54:31.900 --> 54:35.060]  classifier, so in the horse classifier we see green stuff\n",
            "[54:35.100 --> 54:37.580]  on the bottom, because horses are usually on grass, and\n",
            "[54:37.580 --> 54:39.660]  then if you look carefully, the horse actually seems to\n",
            "[54:39.660 --> 54:42.980]  have maybe two heads, one head on each side.\n",
            "[54:42.980 --> 54:46.620]  And I've never seen a horse with two heads, but the linear\n",
            "[54:46.620 --> 54:49.260]  classifier is just doing the best that it can, because it's\n",
            "[54:49.260 --> 54:52.540]  only allowed to learn one template per category.\n",
            "[54:52.540 --> 54:54.940]  And as we see, as we move forward into neural networks\n",
            "[54:54.940 --> 54:58.380]  and more complex models, we'll be able to achieve much\n",
            "[54:58.380 --> 55:00.580]  better accuracy, because they no longer have this\n",
            "[55:00.580 --> 55:03.740]  restriction of just learning a single template per category.\n",
            "[55:05.100 --> 55:09.100]  So if you think about what, another viewpoint of the linear\n",
            "[55:09.100 --> 55:12.900]  classifier is to go back to this idea of images as points\n",
            "[55:12.900 --> 55:16.060]  in a high-dimensional space, and you can imagine that each\n",
            "[55:16.060 --> 55:20.740]  of our images is something like a point in this high\n",
            "[55:20.740 --> 55:24.260]  dimensional space, and now the linear classifier is putting\n",
            "[55:24.260 --> 55:28.140]  in these linear decision boundaries to try to draw linear\n",
            "[55:28.140 --> 55:30.780]  separation between one category and the rest of the\n",
            "[55:30.780 --> 55:33.900]  categories, so maybe up to the point that you're looking\n",
            "[55:34.060 --> 55:37.700]  at, so maybe up on the upper left hand side, we see these\n",
            "[55:37.700 --> 55:41.500]  training examples of airplanes, and throughout the process\n",
            "[55:41.500 --> 55:44.460]  of training, the linear classifier will go and try to draw\n",
            "[55:44.460 --> 55:47.780]  this blue line to separate out with a single line the\n",
            "[55:47.780 --> 55:50.260]  airplane class from all the rest of the classes.\n",
            "[55:50.260 --> 55:52.420]  And it's actually kind of fun if you watch during the\n",
            "[55:52.420 --> 55:54.820]  training process, these lines will start out randomly and\n",
            "[55:54.820 --> 55:56.860]  then go and snap into place to try to separate the\n",
            "[55:56.860 --> 55:57.740]  data properly.\n",
            "[55:59.580 --> 56:02.500]  But when you think about linear classification in this way\n",
            "[56:02.540 --> 56:05.300]  from this high dimensional vector point of view, you can\n",
            "[56:05.300 --> 56:07.780]  start to see, again, what are some of the problems that\n",
            "[56:07.780 --> 56:11.220]  might come up with linear classification, and it's not too\n",
            "[56:11.220 --> 56:14.100]  hard to construct examples of data sets where a linear\n",
            "[56:14.100 --> 56:17.500]  classifier will totally fail, so one example on the left\n",
            "[56:17.500 --> 56:21.180]  here is that suppose we have a data set of two categories,\n",
            "[56:21.180 --> 56:23.740]  and these are all maybe somewhat artificial, but maybe our\n",
            "[56:23.740 --> 56:28.420]  data set has two categories, blue and red, and the blue\n",
            "[56:28.420 --> 56:31.540]  ones, the blue categories are the number of pixels in the\n",
            "[56:31.620 --> 56:34.540]  image which are greater than zero is odd, and anything where\n",
            "[56:34.540 --> 56:36.820]  the number of pixels greater than zero is even, we want to\n",
            "[56:36.820 --> 56:39.140]  classify as the red category.\n",
            "[56:39.140 --> 56:44.660]  So if you actually go and draw what these different\n",
            "[56:44.660 --> 56:47.700]  decision regions look like in the plane, you can see that\n",
            "[56:47.700 --> 56:51.220]  our blue class with an odd number of pixels is going to be\n",
            "[56:51.220 --> 56:54.660]  these two quadrants in the plane, and even will be the\n",
            "[56:54.660 --> 56:56.500]  opposite two quadrants.\n",
            "[56:56.500 --> 57:00.020]  So now there's no way that we can draw a single linear line\n",
            "[57:00.060 --> 57:02.380]  to separate the blue from the red, so this would be an\n",
            "[57:02.380 --> 57:05.860]  example where a linear classifier would really struggle,\n",
            "[57:05.860 --> 57:09.780]  and this is maybe not such an artificial thing after all.\n",
            "[57:09.780 --> 57:12.260]  Instead of counting pixels, maybe we're actually trying to\n",
            "[57:12.260 --> 57:15.660]  count whether the number of animals or people in an image\n",
            "[57:15.660 --> 57:19.700]  is odd or even, so this kind of a parity problem of\n",
            "[57:19.700 --> 57:22.340]  separating odds from evens is something that linear\n",
            "[57:22.340 --> 57:25.940]  classification really struggles with traditionally.\n",
            "[57:25.940 --> 57:29.700]  Another, other problems that you might, other situations\n",
            "[57:29.700 --> 57:33.060]  where a linear classifier really struggles are multimodal\n",
            "[57:33.060 --> 57:38.220]  situations, so here on the right, maybe our blue category\n",
            "[57:38.220 --> 57:40.980]  has these three different islands of where the blue\n",
            "[57:40.980 --> 57:43.980]  category lives, and then everything else is some other\n",
            "[57:43.980 --> 57:47.340]  category, so for something like horses, we saw on the\n",
            "[57:47.340 --> 57:50.060]  previous example, is something where this actually might be\n",
            "[57:50.060 --> 57:53.100]  happening in practice, where there's maybe one island in the\n",
            "[57:53.100 --> 57:55.700]  pixel space of horses looking to the left, and then\n",
            "[57:56.300 --> 57:58.180]  another island of horses looking to the right, and now\n",
            "[57:58.180 --> 58:01.180]  there's no good way to draw a single linear boundary\n",
            "[58:01.180 --> 58:04.900]  between these two isolated islands of data, so any time\n",
            "[58:04.900 --> 58:09.180]  where you have multimodal data, data like one class that\n",
            "[58:09.180 --> 58:12.020]  can appear in different regions of space, is another place\n",
            "[58:12.020 --> 58:15.380]  where linear classifiers might struggle, so there's kind of\n",
            "[58:15.380 --> 58:17.780]  a lot of problems with linear classifiers, but it is a\n",
            "[58:17.780 --> 58:20.820]  super simple algorithm, super nice and easy to interpret\n",
            "[58:20.820 --> 58:24.180]  and easy to understand, so you'll actually be implementing\n",
            "[58:24.180 --> 58:28.340]  these things on your versatile homework assignment.\n",
            "[58:28.340 --> 58:31.940]  So at this point, we've kind of talked about what is the\n",
            "[58:31.940 --> 58:34.580]  functional form corresponding to a linear classifier,\n",
            "[58:34.580 --> 58:38.260]  and we've seen that this functional form of a matrix\n",
            "[58:38.260 --> 58:41.140]  vector multiply corresponds to this idea of template\n",
            "[58:41.140 --> 58:43.300]  matching, and learning a single template for each\n",
            "[58:43.300 --> 58:47.380]  category in your data, and then once we have this weight\n",
            "[58:47.380 --> 58:50.820]  vector, or this trained matrix, you can use it to actually\n",
            "[58:50.860 --> 58:55.020]  go and get your scores for any new training example.\n",
            "[58:56.020 --> 58:58.980]  But what we have not told you is how do you actually go\n",
            "[58:58.980 --> 59:01.980]  about choosing the right W for your data set?\n",
            "[59:01.980 --> 59:04.020]  We've just talked about what is the functional form,\n",
            "[59:04.020 --> 59:06.900]  and what is going on with this thing.\n",
            "[59:06.900 --> 59:11.060]  So that's something we'll really focus on next time,\n",
            "[59:11.060 --> 59:14.260]  and next lecture we'll talk about what are the strategies\n",
            "[59:14.260 --> 59:16.980]  and algorithms for choosing the right W, and this will\n",
            "[59:16.980 --> 59:19.860]  lead us to questions of loss functions, and optimization,\n",
            "[59:19.860 --> 59:21.460]  and eventually contents.\n",
            "[59:21.460 --> 59:25.260]  So that's a bit of the preview for next week,\n",
            "[59:25.260 --> 59:28.020]  and that's all we have for today.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "\n",
        "def group_complete_thoughts(timestamps):\n",
        "    grouped_timestamps = []\n",
        "    current_group = []\n",
        "\n",
        "    for start_time, text in timestamps:\n",
        "        current_group.append(text)\n",
        "\n",
        "        if text[-1] in ['.', '!', '?'] and len(current_group) >= 3:\n",
        "            grouped_timestamps.append((start_time, \" \".join(current_group)))\n",
        "            current_group = []\n",
        "\n",
        "    return grouped_timestamps\n",
        "\n",
        "def format_time(time_str):\n",
        "    h, m, s = map(int, re.sub(r',\\d{3}', '', time_str).split(':'))\n",
        "    return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
        "\n",
        "def srt_to_json(srt_content):\n",
        "    json_data = {\n",
        "        \"dataset_link\": \"https://www.youtube.com/watch?v=OoUX-nOEjG0&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=2\",\n",
        "        \"dataset\": {}\n",
        "    }\n",
        "\n",
        "    pattern = r\"\\d+\\n(\\d{2}:\\d{2}:\\d{2},\\d{3}) --> (\\d{2}:\\d{2}:\\d{2},\\d{3})\\n(.+?)(?=\\n\\d+\\n\\d{2}:\\d{2}:\\d{2},\\d{3} -->|\\Z)\"\n",
        "    matches = re.findall(pattern, srt_content, re.DOTALL)\n",
        "\n",
        "    timestamps = [(format_time(match[0]), match[2].strip()) for match in matches]\n",
        "    grouped_timestamps = group_complete_thoughts(timestamps)\n",
        "\n",
        "    for i, (start_time, combined_text) in enumerate(grouped_timestamps):\n",
        "        json_data[\"dataset\"][start_time] = combined_text\n",
        "\n",
        "    return json.dumps(json_data, indent=2)\n",
        "\n",
        "# Replace this with the actual path to your SRT file\n",
        "srt_file_path = \"/content/Lecture 2 _ Image Classification.srt\"\n",
        "with open(srt_file_path, \"r\") as srt_file:\n",
        "    srt_content = srt_file.read()\n",
        "    json_data = srt_to_json(srt_content)\n",
        "\n",
        "with open(\"transcript1.json\", \"w\") as file:\n",
        "    file.write(json_data)\n"
      ],
      "metadata": {
        "id": "fRVJNAb-mHFx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}